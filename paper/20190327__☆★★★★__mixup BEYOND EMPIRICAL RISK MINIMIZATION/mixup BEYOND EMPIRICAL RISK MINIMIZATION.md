# mixup: BEYOND EMPIRICAL RISK MINIMIZATION

## 甲斐コメント
かの有名なmixup。結果は地味だが理論は結構楽しい。

データとデータの中間みたいなものを生成することで、DataAugmentationができたり、急激に0と1を分けるような圧力がかからなかったりするので、過学習が抑制できる。

また、混在を学習することで誤ったラベルに強かったり、敵対サンプルに対する堅牢性が増加したりする。

疑問点
- 論文では画像を直に足し合わせていたが、FC層を足し合わせたりするとどう機能する？
- その他画像を入れても使える？


「Between-class Learning for Image Classification」の方がよく考察されている気がするのでそっち読むの推奨。



## Abstruct
DNNは強力ですが、暗記や敵対的な例に対する感受性など、望ましくない動作を示します。この研究では、これらの問題を軽減するための単純な学習原則である、ミックスアップを提案します。mixupは例の対とそれらのラベルの線形結合についてニューラルネットワークを訓練します。
ミックスアップはニューラルネットワークを正規化して、トレーニング例の間の単純な線形の振る舞いを支持します。 ImageNet-2012、CIFAR-10、CIFAR-100、GoogleコマンドおよびUCIデータセットに関する我々の実験は、ミックスアップが最先端のニューラルネットワークアーキテクチャの一般化を改善することを示しています。 また、ミックスアップによって、破損したラベルの記憶が減り、敵対的な例に対する堅牢性が高まり、生成的な敵対的ネットワークのトレーニングが安定することがわかりました。

## 1 Introduction
現在成果を上げているDNNは２つの共通点を共有している。
- 経験的リスク最小化（ERM）原理としても知られている学習則である、訓練データに対する平均誤差を最小にするように訓練されている
- これらの最先端のニューラルネットワークのサイズは、トレーニング例の数に比例して増減します。

これらは学習理論における古典的な結果に矛盾する。
- 学習機械のサイズが訓練データの数と共に増加しない限り、ERMの収束が保証される。学習器のサイズは、そのパラメータ数、またはそれに関連してそのVCの複雑度によって測定される

最近の研究：「ERMで良いのか？」という疑問
- 一方で、ERMは、強い正則化がある場合や、ラベルがランダムに割り当てられている分類問題の場合でも、大規模ニューラルネットワークが（一般化する代わりに）トレーニングデータを記憶することを可能にする
- 他方で、ERMでトレーニングされたニューラルネットワークは、トレーニングの分布のすぐ外側の例で評価すると、予測が劇的に変化します。これは、敵対的な例としても知られています。この証拠は、ERMがトレーニングデータとわずかに異なるテスト分布について説明したり一般化したりすることができないことを示唆しています。
- しかし、ERMに代わるものとは？

訓練データと類似しているが異なる例を訓練するための選択の方法：データ拡張
- Vicinal Risk Minimization（VRM）の原理（Chapelle et al。、2000）によって定式化
- トレーニングデータの各例の周囲を人間の知識で記述する必要があります。
- 汎化の改善につながるが、この手順はデータセットに依存しているため、専門知識を使用する必要がある。
- 近傍の例は同じクラスを共有すると仮定し、異なるクラスの例にまたがって近傍の関係をモデル化することはありません。

mix-up
- 単純でデータにとらわれないデータ拡張ルーチン
- 特徴ベクトルの線形補間が関連するターゲットの線形補間につながるはずであるという以前の知識を取り入れることによってトレーニング分布を拡張する。
- 数行のコードで実装でき、最小限の計算オーバーヘッドで済む。

論文の構成
- CIFAR-10、CIFAR-100、およびImageNet-2012の画像分類データセットにおいて新しい最先端の性能を可能にします（3.1および3.2）。
- 不正なラベルから学習するとき（3.4節）、または敵対的な例に直面するとき（3.5節）、DNNの頑健性を高めます。
- スピーチ（3.3）と表（3.6）データの一般化を改善し、GANのトレーニングを安定させるために使用することができます（3.7）。
- 先行研究との関連性を調査し（4）、また議論のポイントをいくつか提示します（5）。

CIFAR-10実験を再現するコード
- https://github.com/facebookresearch/mixup-cifar10

## 2 ERMからmixupへ （ここ基本的に意訳）
ERM
- x, yの分布Pは事前にはわからんけど、データxi, yiなら取得できる
- f(xi)とyとの差をempirical riskとして定義して、それを最小化しよう
- 問題：empirical riskを最小化するために丸暗記をして、学習データの外側でfの望ましくない振る舞いを引き起こす

mixup
- 2サンプルの中間みたいなのを出す（周辺分布）、ちなみに3サンプルだとそんなに良くならない
- モデルfが訓練例の間で線形に振舞うように促すデータ増強の形式として理解することができる
- 訓練例の外側を予測するとき、この線形の振る舞いは望ましくない振動の量を減らす
- Fig.1b：クラスからクラスへと直線的に推移する決定境界を導き、不確実性のより滑らかな推定を提供する
- Fig.2：ミックスアップで訓練されたモデルは、訓練サンプル間のモデル予測と勾配ノルムに関してより安定している

### 3 結果
つまらないので省略
