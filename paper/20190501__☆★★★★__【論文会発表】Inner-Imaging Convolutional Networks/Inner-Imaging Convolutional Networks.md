# Inner-Imaging Convolutional Networks



## 甲斐コメント

- Attention構造の改良。
- チャネル「間」の関連性をちゃんと拾おうよというテコ入れをしているという認識でOK
- Attention自体に関する疑問というか調査したい部分として、途中の括れている部分って各特徴の良好なEncodingになってたりする素敵な効果があって、それ抽出したらめっちゃ良好な特徴ベクトル群できないかな？
- Residualの部分にちょっと工夫するのがトレンドなのかな？



## Abstract

DNNの課題

- 深刻な計算コスト、冗長性
  - チャネルの多様性によってカバーしてきた
- 相補性、完全性
  - カバーできていない

新しい内部イメージングアーキテクチャ

- グループ内、グループ間の両方の関係を同時にモデル化するために、畳み込みカーネルを使用してフィルタ信号点をグループに編成
- チャネルの多様性が増す
- 相補性と完全性も明確に向上
- 軽量
- モデリングの効率とパフォーマンスを向上させるために実装するのが簡単
- residualを持つNWでInner Imagingアーキテクチャの有効性を検証



## 1. Introduction

過去の研究ではチャネル間の組み合わせ、相補性を明示的にモデル化していなかった

提案手法：Inner-Imaging

- 最初に特徴信号を n * m次元の画像に再配置し、それをスキャンするグループ化フィルタ w(n×m) を配置
  - 同じ受容野内のチャネル信号 uij は複数の方向（上下左右、ナナメなど）から関係を構築し、デフォルトでグループに割り当てらる
  - 同じグループ内のチャネル信号は次の段階に行く前にcondenseされる
  - 捕捉された特徴は互いに相補的である
- グループごとの関係をモデル化するFCレイヤを採用
  - グループ化フィルタの形状を調整することで、チャネルグループのサイズを柔軟に制御
  - マルチスケールフィルタを統合することで、チャネルリレーショナルモデリングをより完全かつ正確にする
  - 残差マッピングがアイデンティティフローの補完的モデリングであることを考慮して、アイデンティティマッピングをチャネル関係モデリングの入力に導入し、アイデンティティフローが残差チャネルの重み付けを変更することを可能にする
  - 畳み込みチャネルの完全性を高める

貢献：

- Inner-Imaging構造
  - フィルタを使用して、再配列されたInner Image Map上のチャネル信号を上手くまとめる
  - 同時に畳み込みチャネルに対するグループ内協調とグループごとの相補的関係をモデル化する
  - より完全で説明的なChannel-wise Attentionである
- Identity Mapping
  - residual channel attention のモデリング
  - 各residual channel から自律的に相補性を決定する

- Inner Imaging の数パターンを調査
  - 既存のCNN構造に基づく構成
  - アブレーションテストで、提案された各メカニズムの有効性を評価



## 2. Related Work

###### 効率的な畳み込み構造

- ネットワーク特徴に正則化制約
- 中間特徴のランダムなオクルージョン, 摂動
- 過度な畳み込みアーキテクチャのブロック, チャネルのpruning
- 早期終了
- 既存のコンポーネントと機能を効率的に使用
- 特徴マップを密にモデル化
- 特徴間の関係のモデル化

###### Attention

- Spatial Attention Areaのモデル化
- リソースの割り当てを偏らせるためのツールとして、CNNの内部特徴を規制するためにも使う
- Channelwise Attention：Spatial Attentionと組み合わせて使う

→互いに補完するために特徴を集約するか、単純なエンコーダの後で特徴マップの多様性を高めるか

→→Inner-Imaging は両方を同時に考慮

→→→チャネルを組織化するために畳み込みフィルタを使用

→→→マルチスケールでグループ化協調関係を明示的に反映

→→→→CNNチャネルの多様性、相補性および完全性の統合最適化を達成



## 3. Method

###### channel-wise attention

- residualとidentityのGAPしたものを横に並べてFC

###### Inner Image Mapping

- channel-wise attentionのGAP直後部分に挿入
- 2xCに並べてconv (これがinner image map)
  - もっとも簡単なパターンではε個の2x1フィルタ
  - より一般的には (ε個) x (nサイズ) のフィルタでconvしてFlatten

###### Inner Image Mappingの発展形 (folded mode)

- Inner Image Mappingをreshapeしたものをconv
  - フィルタの形状を制限したくない
  - 隣接するフィルタのみしか関連性を見られないのはおかしい
  - SkipConnectionやidentityMappingSignalsがないCNNでも機能するようにしたい
- residualとidentityの交互配置を維持しながら n x m に折りたたむ（なるべく正方形っぽくする）
  - 本当に折りたたむ感じでnpのreshapeみたいな感じの実装かな



## 4. Experiments

###### Ablation Study on CIFAR

同じバックボーン上に徐々に追加することにより検証

- identityとresidualからInner Image Mappingをする（D）
- マルチスケールグルーピングフィルタを使う（A）
- Inner Image Mappingを折りたたむ（F）

→追加するごとにミスが減少

![キャプチャ](画像\キャプチャ.PNG)

###### SOTAとの比較

![キャプチャ2](画像\キャプチャ2.PNG)

PyramidNetにInner-Imagingを適用

→SOTAと同じくらい（DMRNet）

→適用前と比較すると性能向上がみられる



###### channel-wise attentionの出力

モデリングが深くなるにつれて、チャネルアテンションの出力は振動するようになる

- 最も深い層では、InI-Netsのアテンション出力はSE-Netよりかなり低い
- いくつかのより深い層では、In-Netsの注意出力はSE-Netsより永続的な振動強度を示す

→深層におけるresidual写像の冗長モデリングを効果的に抑制

→深層でのchannel-wise attentionの活動をよりよく維持



###### 普遍性

- 複数のたたみ込みバックボーンと複数のデータセット上で普遍的
- スケーラブル
- チャネルのグループ化された関係のモデリングは、多様なタイプのグループ化フィルタを選択できる

→小規模ネットワークに組み込んで大規模ネットワークに匹敵するパフォーマンスを達成



## 5. Conclusion

InnerImaging

- CNNのチャネル関係をモデル化するための新しい構造

- 畳み込みフィルタを用いてチャネルのグループ化関係を整理
- グループ内協調とグループ間相補チャネル関係の両方を明示的にモデル化
- 有限スケール下の畳み込みネットワークのモデリング効率を効果的に改善
- 拡張可能で使いやすい
- 複数のベンチマークデータセットで検証



