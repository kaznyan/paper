# SGD ON RANDOM MIXTURES PRIVATE MACHINE LEARNING UNDER DATA BREACH

## 甲斐コメント

混合のアルゴリズムが良くわからない……mixupみたいな感じなのかな？いうて（平均）2枚の合成が良かったと言ってるあたりもmixupと一致する知見だし

ただ少し議論が雑なのと結果が少ないのできっちりベースにする気にはならないなあ

それこそmixupちゃんと読んだ方が良い


## Abstruct

データ侵害の脅威にさらされているデータを保護するための簡単な方法として、ランダム混合による確率勾配降下法（SGDRM）を提案します。 SGDRMはディファレンシャルプライベートでありながら線形活性化によるディープニューラルネットワークのための世界的に最適な点に収束することを示した。 また、訓練データとしてプライベート混合を用いて非線形ニューラルネットワークを訓練し、SGDRMの実用性を証明します。

## 1 Introduction
機械学習のトレーニングデータセットは機密データであり、プライバシーを犠牲にすることなく訓練する必要がある。これに対して様々な攻撃が検討されていて、例えばブラックボックス攻撃やホワイトボックス攻撃がある。

この研究で扱うより強力な攻撃シナリオとして「データ侵害攻撃」モデルを想定する。データ侵害攻撃モデルでは、攻撃者は学習アルゴリズムに送られている入力データセットにアクセスする。このモデルでデータを保護するために、データ所有者は元のデータセットを別の形式に変換し、変換されたデータセットを使ってトレーニングアルゴリズムを提供する必要があります。さらに、変換されたデータセットを使用してもモデルを効率的に学習できるトレーニングアルゴリズムも必要です。

我々の解決策として、我々はデータ発行アルゴリズムとトレーニングアルゴリズムからなる確率的混合降下確率分布（SGDRM）を提案する。重要な考え方は簡単で「トレーニングデータポイントをランダムに混合し、それらに対して標準のSGDアルゴリズムを実行する」ことです。

私たちの計画のデータ公開アルゴリズムは以下の通りです：データ所有者はデータセット全体の無作為な部分集合を選び、そして付加ノイズを含む無作為線形結合を公開します。 

![Fig1](C:\Users\d184813\Desktop\論文\おわ\20190329__☆☆☆★★SGD ON RANDOM MIXTURES PRIVATE MACHINE LEARNING UNDER DATA BREACH\Fig1.PNG)

このようなデータポイントのランダムアフィンの組み合わせ、またはランダム混合が与えられると、学習者はそれらに対して通常のSGDアルゴリズムを実行します。 本研究では、1）加法性雑音を含むランダム混合は微分非公開であり、2）SGDは線形ニューラルネットワークの大域的最適値に収束することを理論的に示す。 さらに、我々は経験的に我々のアルゴリズムがディープコンボリューションニューラルネットワーク（ＣＮＮ）のような一般的なニューラルネットワークに対してもうまく働くことを示す。

## 2. 理論
機密性の高いトレーニングデータセットを保持していて、公開データセットが元のデータセットに関するあまり多くの個人情報を漏らさないように処理済みデータセットを公開したいデータ所有者を考えてみましょう。 同時に、学習者は公開されたデータセットに対して効率的な学習アルゴリズムを実行する必要があります。

私たちの目標は、上記の要件を満たすために、データ所有者のためのプライベートデータ公開メカニズムと学習者のための効率的なトレーニングアルゴリズムを共同で設計することです。 この研究では、n個のデータポイントを持つ標準的な教師あり学習の設定に焦点を当てます。

このアルゴリズムはランダムに選ばれたデータ点をランダムな係数と混合することによってプライバシーを達成する。 各混合に含まれるデータ点の平均数を lで表し、それを混合幅と呼びます。 公開されるデータポイントの数をTとすると、データ所有者はデータポイントのT個のランダムな線形結合を生成します（ラベルと特徴の双方が結合されます）。



## 3. 結果
一般的なニューラルネットワークに対するその振る舞いを観察するために、MNIST、CIFAR10、およびSkin Lesionに対してSGDRMを実行します。
ノイズのない一様なランダム係数を用いて混合物を生成します。 図１に示されているのはサンプル混合物である。それらは、lが増加するにつれて人間の目には完全にランダムに見えるようになります。 分類ネットワークでは、2つの畳み込み層とそれに続く3つの完全にFC層（100; 100; 10）を使用します。
標準的なSGDの代わりに計算効率のために、サイズ8のミニバッチを持つAdamオプティマイザを使用します。表1は結果をまとめたものです。 これらのパフォーマンスは、元の「混合されていない」テストデータセットに対するものです。 我々は、 l= 2のSGDRMの性能が標準のSGDよりわずかに優れていることを観察した。 （２０１８）。 その後、lが大きくなるにつれて、パフォーマンスは低下し始めますが、非公開になります。