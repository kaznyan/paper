# Ensemble of Convolutional Neural Networks Trained with Different Activation Functions



## 甲斐コメント

- RelatedにあったSwish関数もちゃんと勉強したい

- 論文の内容はめっちゃ簡単で、活性化関数を変えてアンサンブルするだけでうまく働くってこと

- 他に変えられる要素（具体的には思いつかないけど）に拡張してもっとアンサンブルできるよね

  要は結局アンサンブル最強なんだよなあ

- Mexican Hat関数はめっちゃ推してるけど言うほどでもないやろなあ

  ぶっちゃけ本筋からすると余計

  でも他と性質の違う関数ならアンサンブル的には貢献するかもね

- アンサンブルってどうやってるんだろ

  ここの如何によっては特徴抽出とFC層の学習をバラバラにした超イージーモードでも学習できないかも



## Abstract

効率的で機能的な活性化関数が重要で、特に勾配消失を避けることが重要

提案：

- 複数の活性化関数を使って訓練されたCNNのアンサンブル
- 新規の活性化関数の提案

検証：

- Vgg16とResNet50、10以上のデータセットでCNNの性能を検証
- ReLUで訓練されたCNNよりも優れていることを示した

MATLABコード：https://github.com/LorisNanni



## 1. Introduction

- 活性化関数といえばReLU

- ReLUに似た関数の研究に注目が集まってきた
  - Leaky ReLU
  - ELU
  - SELU
  - PReLU
  - APLU
    - 活性化関数内のパラメータを学習に含む
    - 今回の提案手法に最も近い
- 学習可能な活性化関数について
  - 複数の固定アクティベーションを使用して定義することもできる
  - ManessiとRozza
    - tanh、ReLU、および identityのアフィン的な組み合わせを学習することによって、学習可能な新しいアクティベーション関数を作成
  - Swish活性化関数 𝑓(𝑥) = 𝑥𝜎(𝑥) 
    - **強化学習を用いて**この活性化機能を見出した
- 提案１：活性化関数（AF）のアンサンブルが、単独よりも優れていることを示す
- 提案２：新しい活性化関数　Mexican ReLU（MeLU）
  - PReLUと複数のMexican hat 関数の和である区分的線形活性化関数
  - ゼロから無限大まで及ぶ多くのパラメータを持つ
  - パラメータの数が無限大になると、コンパクト集合上のすべての連続関数を近似できる
  - どの方向にも飽和せず、その勾配はほとんど平坦にならない
  - パラメータを変更しても、Activationはほんの少しの間隔でしか変更されないため、最適化プロセスが簡単になる

## 2. Related Work

- 活性化関数の紹介。暇あったら読む



## 3. 提案手法：MeLU

- Mexican Hat関数を定義

![キャプチャ](画像\キャプチャ.PNG)

![キャプチャ2](画像\キャプチャ2.PNG)

- MeLUはPReLUと複数のMexican hat 関数の和

  ![キャプチャ3](画像\キャプチャ3.PNG)

  - 隠れ層の各チャンネルに対して用いられる

  - cj は学習可能、aj, 𝜆𝑗 は固定

  - Mexican Hat の各パラメータは "maxInput" から再帰的に決定する

    ![キャプチャ4](画像\キャプチャ4.PNG)

- MeLUの性質
  - ReLU、Leaky ReLU, PReLUの拡張である
  - すべての𝑐𝑖がゼロに初期化されている場合、MeLUはReLUと一致
    - ReLU, Leaky ReLU, PReLU などで事前トレーニングされたNWの転移学習に役立つ
  - kが無限大になるにつれて𝐿2（[0,1024]）内のすべての関数を近似することができる
  - APLUに似ているが、上位互換的な性質である（詳細は読んでもよく理解できなかった）
  - パラメータが多いので過学習しやすいかも



## 4. Experimantal Results

![キャプチャ5](画像\キャプチャ5.PNG)

- 新しい活性化関数のテストをした

  →他のReLU派生と並ぶ程度の高精度

- アンサンブルしたものの精度をテストした

  - ENS：MaxInputを1か255のどちらかに固定したものをアンサンブル
  - eENS：MaxInputを1にしたものと255にしたものの両方をアンサンブル

  →ほとんどの場合でアンサンブル前を上回る高精度



## Conclusion

- 新しい活性化関数を提案
- 活性化関数のみが異なる複数のCNNのアンサンブルが単一のCNNの結果よりも優れている
  - 他のものより一貫して優れている活性化関数はない
  - MeLUは他の活性化機能と競合する
  - MeLUは、特にVGG16において、𝑘= 4のときに最も性能が高い
  - 欠点は、速度とメモリの要件