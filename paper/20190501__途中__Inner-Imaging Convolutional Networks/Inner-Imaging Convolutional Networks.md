# Inner-Imaging Convolutional Networks



## 甲斐コメント



## Abstract

DNNの課題

- 深刻な計算コスト、冗長性
  - チャネルの多様性によってカバーしてきた
- 相補性、完全性
  - カバーできていない

新しい内部イメージングアーキテクチャ

- グループ内、グループ間の両方の関係を同時にモデル化するために、畳み込みカーネルを使用してフィルタ信号点をグループに編成
- チャネルの多様性が増す
- 相補性と完全性も明確に向上
- 軽量
- モデリングの効率とパフォーマンスを向上させるために実装するのが簡単
- バックボーンとして残余ネットワークを持つ内部イメージングアーキテクチャの有効性を検証



## 1. Introduction

過去の研究ではチャネル間の組み合わせ、相補性を明示的にモデル化していなかった

提案手法：Inner-Imaging

- 最初に特徴信号を n * m次元の画像に再配置し、それをスキャンするグループ化フィルタ w(n×m) を配置
  - 同じ受容野内のチャネル信号 uij は複数の方向（上下左右、ナナメなど）から関係を構築し、デフォルトでグループに割り当てらる
  - 同じグループ内のチャネル信号は次の段階に行く前にcondenseされる
  - 捕捉された特徴は互いに相補的である
- グループごとの関係をモデル化するFCレイヤを採用
  - グループ化フィルタの形状を調整することで、チャネルグループのサイズを柔軟に制御
  - マルチスケールフィルタを統合することで、チャネルリレーショナルモデリングをより完全かつ正確にする
  - 残差マッピングがアイデンティティフローの補完的モデリングであることを考慮して、アイデンティティマッピングをチャネル関係モデリングの入力に導入し、アイデンティティフローが残差チャネルの重み付けを変更することを可能にする
  - 畳み込みチャネルの完全性を高める

貢献：

- Inner-Imaging構造
  - フィルタを使用して、再配列された内部イメージマップ上のチャネル信号を上手くまとめる
  - 同時に畳み込みチャネルに対するグループ内協調とグループごとの相補的関係をモデル化する
  - より完全で説明的なチャンネル別アテンション方法である
- Identity Mapping
  - residual channel attention のモデリング
  - 各 residual channel から自律的に相補性を決定する

- Inner Imaging の数パターンを調査
  - 既存のCNN構造に基づく構成
  - アブレーションテストで、提案された各メカニズムの有効性を評価



## 2. Related Work

###### 効率的な畳み込み構造

- ネットワーク特徴に正則化制約
- 中間特徴のランダムなオクルージョン, 摂動
- 過度な畳み込みアーキテクチャのブロック, チャネルのpruning
- 早期終了
- 既存のコンポーネントと機能を効率的に使用
- 特徴マップを密にモデル化
- 特徴間の関係のモデル化

###### Attention

- Spatial Attention Areaのモデル化
- リソースの割り当てを偏らせるためのツールとして、CNNの内部特徴を規制するためにも使う
- Channelwise Attention：Spatial Attentionと組み合わせて使う

→互いに補完するために特徴を集約するか、単純なエンコーダの後で特徴マップの多様性を高めるか

→→Inner-Imaging は両方を同時に考慮

→→→チャネルを組織化するために畳み込みフィルタを使用

→→→マルチスケールでグループ化協調関係を明示的に反映

→→→→CNNチャネルの多様性、相補性および完全性の統合最適化を達成



## 3. Method

ここちゃんと読まないと全然わからん。。。



## 4. Experiments

###### Ablation Study on CIFAR

同じバックボーン上に徐々に追加することにより検証

- 識別写像と残差写像からの二重流共同符号化（D）
- マルチスケールグルーピングフィルタ集約（A）
- 折り畳まれた内側画像マップ（F）

→コンポーネントを追加するとミスが減少

![キャプチャ](画像\キャプチャ.PNG)

###### SOTAとの比較

PyramidNetにInner-Imagingを適用

→SOTAと同じくらい

→適用前より良い結果



###### チャネルごとのAttentionの出力

モデリングが深くなるにつれて、チャネルアテンションの出力は振動するようになる

- 最も深い層では、InI-Netsのアテンション出力はSE-Netよりかなり低い
- いくつかのより深い層では、In-Netsの注意出力はSE-Netsより永続的な振動強度を示す

→深層における残差写像の冗長モデリングを効果的に抑制

→深層でのチャネルごとのAttentionの活動をよりよく維持



###### 普遍性

- 複数のたたみ込みバックボーンと複数のデータセット上で普遍的
- スケーラブル
- チャネルのグループ化された関係のモデリングは、多様なタイプのグループ化フィルタを選択できる

→小規模ネットワークに組み込んで大規模ネットワークに匹敵するパフォーマンスを達成



## 5. Conclusion

InnerImaging

- CNNのチャネル関係をモデル化するための新しい構造

- 畳み込みフィルタを用いてチャネルのグループ化関係を整理
- グループ内協調とグループ間相補チャネル関係の両方を明示的にモデル化
- 有限スケール下の畳み込みネットワークのモデリング効率を効果的に改善
- 拡張可能で使いやすい
- 複数のベンチマークデータセットで検証



