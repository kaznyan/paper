# ADVERSARIAL EXAMPLES IN THE PHYSICAL WORLD

## 甲斐コメント
だいたいAbst読めばこと足りたのでそれ以降は読んでない。

余談、Adversarial Logit Pairing の方にも書いたように、興味のわかない分野である。間違うようにノイズを加えて「どうだ間違っただろ」と言って何が楽しいのか？

人間にも得手不得手があるようにAIにも得手不得手がある、ってだけじゃダメなのかな？


## Abstruct
既存のほとんどの機械学習分類器は、敵対的な例に対して非常に脆弱です。敵対的な例は、機械学習分類器にそれを誤分類させることを意図した方法で非常にわずかに修正された入力データのサンプルです。多くの場合、これらの修正は非常に微妙なので、人間の観察者はその修正にまったく気付かないかもしれませんが、それでも分類器は間違いを犯します。

敵対者の例は、たとえ敵対者が基礎となるモデルにアクセスできない場合でも、機械学習システムに対する攻撃を実行するために使用される可能性があるため、セキュリティ上の懸念をもたらします。今までのところ、これまでの研究はすべて、敵対者が機械学習分類器に直接データを送り込むことができる脅威モデルを想定していました。

これは、物理的な世界で動作しているシステム、たとえばカメラや他のセンサーからの信号を入力として使用しているシステムなどには必ずしも当てはまりません。この論文は、そのような物理的な世界のシナリオにおいてさえ、機械学習システムは敵対的な例に対して脆弱であることを示しています。

携帯電話のカメラから得られた敵対的な画像をImageNet Inception分類器に送り、システムの分類精度を測定することによってこれを実証します。カメラを通して見た場合でも、敵対的な例の大部分は誤って分類されていることがわかります。
