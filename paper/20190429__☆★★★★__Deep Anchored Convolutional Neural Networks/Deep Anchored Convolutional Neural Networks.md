# Deep Anchored Convolutional Neural Networks



## 甲斐コメント

- 重み共有のConv層をめっちゃ重ねちゃえばパラメータ数ほとんど多くならないから深くできるやんという発想。それをRegulatorとか入れる工夫によって実際に実用レベルに乗せてしまった恐ろしい論文である。

- ものすごく精度が良いというわけではない。ただし同程度の精度のNWと比較すると驚異的にサイズが小さい。要はコスパがいい。
- ガチ学習してから蒸留とかするより最初からこれでやればええやん？　というのが1つの主張ポイント。



## Abstract

Deep Anchored Convolutional Neural Network (DACNN)

- レイヤー間で単一の畳み込みカーネルを使用してスタックされたネットワーク
- メモリコストを気にせずに深いDACNNを構築
- アーキテクチャのパフォーマンスを向上させる工夫
  - 部分共有ウェイトネットワーク（DACNN-mix）
  - 簡単にプラグイン可能なモジュール（コインドレギュレータ）
- メモリを節約しながら、CIFAR-10、CIFAR-100、SVHNで高精度のパフォーマンスを維持



## 1. Introduction

典型的なCNNの圧縮手法：「減算」方式

- まず学習して不要なフィルタの一部を削除
- 元のアーキテクチャが設定されるとモデルのパフォーマンスはほとんど向上しない
- モデルをさらに改良する必要がある場合元のアーキテクチャーを再構築してプルーニングを再実行

提案手法：「追加」方式のDeep Anchored Convolutional Neural Network (DACNN)

![キャプチャ](画像\キャプチャ.PNG)

- 単一の畳み込みカーネルをすべてのレイヤーにまたがって積み重ねる新しいアーキテクチャー
- Poolingレイヤー間で部分的に重みを共有
- 「簡単プラグイン」方式
  - DACNN基本モデルにいくつかのパラメータを追加してパフォーマンスを向上
  - 追加パラメータの数はモデル設計者によって決定される
  - モデルサイズとモデル性能の間のトレードオフを容易に制御
  - 既存のアーキテクチャのほとんどに適用



## 2. Related Work





## 3. Anchored Weights Convolution

- 重み共有

  - Plain：全部共有
  - Mixed：全部共有ではなく大きさごとに立てる（全部で4種類くらい立てる）

- Regulator![キャプチャ2](画像\キャプチャ2.PNG)

  

  - DACNNのパフォーマンスを向上させるための簡単なプラグイン
  - 1×1 Conv、BN、ReLU
  - Regulator内のパラメータは共有しない



## 4. Experiments

- Plain DACNN

  - 深くしたりres構造を導入することでサイズはほとんど変わらないままに性能向上
  - しかし割と限界はある

- mixed DACNN

  - そこでチャネルが拡大するたびに追加のパラメータを導入する、mixed DACNN

    （ResNetベースでは、ショートカットのごとに1×1の畳み込みがもう1つ必要）

  - 非常に少ない数のパラメータで同数の層を有するVGG, ResNetに匹敵

  - DACNNが深いほど改善の余地がある

- Regulator
  - 1×1 Conv なのでネットワークに追加されるパラメータはごくわずか
  - セクションごとにレギュレータを追加することによって性能改善
  - ネットワークが深いほど活きる
- まとめ：めっちゃ効率的![キャプチャ3](画像\キャプチャ3.PNG)



## 5. Conclusion

DACNNでモデルを圧縮

DACNNの性能を向上させる2つの方法

- 混合構造を選択的に適用
- レギュレータによって制御

→一般的なアーキテクチャと比較してはるかに少ないパラメータで同様の性能を得ることができる



