# Dense Scene Information Estimation Network for Dehazing



## 甲斐コメント

- 曇り画像除去のSOTA
- パラメータの推定、単純な復元という2つのアプローチが考えられるなか、2つ同時にやってしまった方法
- 迷ったらこういう切り口もこの先使えるのかも？



## Abstract

Image Dehazing

- 今までの手法例：

  - 周囲光（Ａ）および透過率マップ（ｔ）を推定するディープネットワークを設計
  - ヘイズモデルの逆行列を使用して、デヘイズ画像を推定

- 提案手法：2つの新しいネットワークアーキテクチャを開発

  - At-DH

    - シーン情報を共同で推定するために、共有DenseNetベースのエンコーダと2つの異なるDensetNetベースのデコーダを設計（A, t）

      →物理的パラメータを別々に推定する過去の取り組みとは対照的

  - AtJ-DH

    - At-DHの自然な拡張
    - A, t に加え、曇りのない画像を再作成するためDenseNetベースのデコーダを追加

- 特に濃い曇りによって損なわれた画像を回復するとき、At-DHとAtJ-DHが最先端の選択肢を上回ることを実証



## 1. Introduction

- 古典的なヘイズモデル

  ![キャプチャ](画像\キャプチャ.PNG)

  　I：観測されたぼやけた画像

  　J：真のシーンの放射輝度

  　A：周囲光の強度

  　t：透過率マップ

  さらに

  ![キャプチャ2](画像\キャプチャ2.PNG)

  　β：大気の減衰係数

  　d：シーン深度

  Image Dehazingは、Iに基づいてJを回復するプロセス

  - 式１から、入力として任意の所与のかすんでいる画像に対する解決策の選択に関して複数の可能性がある

  - dに関する情報が提供されれば、Jを復元する作業が簡単だが、この深さ情報が実際に利用可能であることはめったにない

- DLを用いた方法
  - IとJの関係を学習する
  - IとJがセットで手に入らないことも多いので合成データセットを使うことが多い
- ちなみに代表的なDatasetは、NTIRE2019-Dehaze Challenge
- 提案手法
  - At-DH：3位
  - AtJ-DH：1位



## 2. Related Work

ディープラーニングベースの方法

- Cai
  新しいBReLUユニットでtを推定するためにエンドツーエンドのCNNネットワークを導入

- Ren

  tを推定するためのマルチスケールディープニューラルネットワークを提案

- Li

  他の本質的な情報を導入することなく、CNNフレームワークを使えば良いという手法からの脱却

  オールインワンの曇り除去ネットワークを提案（tとAを1つの変数にエンコード）

GANによる方法例

- 推定されたｔと曖昧さ除去された結果との間の相互構造情報を組み入れるために、曇り除去画像および推定された透過マップが本物か偽物かを決定する同時識別器
- DenseブロックおよびResブロックに基づく知覚ピラミッドDNNを使用するマルチスケールDehazing手法
  - 復号化中にシーンのコンテキスト情報を組み込むために復号器内にピラミッドプーリングモジュールを有する符号器 - 復号器構造を含む
- 特別に設計されたFCNに基づいてtとAを推定するためのbi-directional consistency loss
- bilinear CNNを使用して、t, A, Jの相関関係を直接モデル化するbilinear lossを用いてt, Aを推定



## 3. 提案手法

- 共有エンコーダと複数のデコーダで構成
  - DenseNetブロックでエンコーダとデコーダを構築
  - エンコーダは一般的な特徴抽出器
  - デコーダはエンコーダから抽出された特徴に基づいてシーン情報を推定

#### 3.1 Building Block

- エンコーダ
  - Densely Connected Networkの前半部分のみを利用する
- デコーダ
  - 構造はエンコーダと同様だが、より多くのBNを有する
  - 連続するDenseブロック間にResブロックを導入してより高周波の情報を入手する
- Refinementブロック
  - 異なるスケールで出力を改良するために使用
  - 様々なサイズのLocal Average Poolingを行い、1x1poolingでまとめ、アップサンプリングした後に仕上げの畳み込みを行う

#### 3.2 At-DHとAtJ-DHの概要

![キャプチャ3](画像\キャプチャ3.PNG)

モデル完成後は、「推論結果 = Dehazeされた画像 = I を入力して A や t を使って計算した J」とする



#### 3.2 At-DH

Lossの計算方法

1. 正しいIを入力してAとtを推論します（Iを作る元となったJもありますがここでは表に出てきません）

2. これを使ってJを計算できます

   ![キャプチャ4](画像\キャプチャ4.PNG)

3. このJを使ってIをさらに逆推論します

   ![キャプチャ5](画像\キャプチャ5.PNG)

4. ここまでで、正しいIとJ、推論されたIとJが出てきたことになります

5. Lossの第一項：正しいIとJ、推論されたIとJがどれだけ離れているかをPixelレベルで見る。

   いわゆるReconstruction Lossであり、二乗誤差で計算する

   ![キャプチャ6](画像\キャプチャ6.PNG)

6. Lossの第二項：正しいIとJ、推論されたIとJが大域的に似ているのかを特徴の比較により見る。

   VGG16により抽出した特徴同士の二乗誤差で計算する（VGGはトレーニングには使っていない部外者っぽい？）

   ![キャプチャ7](画像\キャプチャ7.PNG)

7. 足します

   

#### 3.3 AtJ-DH

At-DHの弱点は、ほとんどの情報が視覚的に隠されている領域に対してAとtのためのデコーダが十分でないこと

→新しいJデコーダを導入することでAtモデルを拡張

→→かすんでいる観測とそれに対応するGTをペアとして扱うことによってトレーニングされる

→→→トレーニングプロセス中に実際のシーン情報を提供することができる

Lossの計算方法

1. Training Decoder.A and Decoder.t：

   At-DHとほぼ同様のLossをAとtについて計算します

2. Training Decoder.J：

   Decoder J によって直接生成したJについて、At-DHと同様のLossを計算します

3. Training Decoder.A, Decoder.t, and Decoder.J together：

   Aとtの推論値から計算で求めたJと生成したJがどの程度離れているかを二乗誤差で計算します

4. 足します



## 4. Dataset & Training

#### Dataset

NTIRE2019- Dehazeデータセット

- プロの霧発生器を含むプロ用カメラによって収集

- 45のぼんやりした画像（屋内と屋外の両方の環境で濃い霞が生成されたもの）と、まったく同じシーン情報を持つ対応するグラウンドトゥルース（曇りのない）画像で構成

NTIRE2018- Dehazeデータセット

- 霧が薄いので2019と同レベルになるように補正



#### Training

###### Step1

- エンコーダを事前トレーニング

###### Step2

- Step1から学習したエンコーダ、A, t, Jを推定するために新しく構築したデコーダを組み合わせる
- 最初の50エポックについては、トレーニングデータはステージ1と同じ
- 次の70エポックについては、トレーニングデータはNTIRE19からのみ



#### IRCNN Post-processing

結果を視覚的にさらに改善するために、σ= 15のオプションのIRCNN denosierを後処理ユニットとして組み込む



## 5. Result

性能を定量化するために使用される評価指標

- ピーク信号対雑音比（Peak Signal-to-Noise Ratio, PSNR）
- 構造類似性指数（Structural Similarity Index, SSIM）

結果

![キャプチャ8](画像\キャプチャ8.PNG)



## 6. Conclusion

- 共有のDenseNetエンコーダと2つの異なるDensetNetデコーダを使用して、シーン情報 (A, t) を同時に推定

- At-DHの自然な拡張としてAtJ-DHネットワークを開発
  - Aとtと共に曇りのない画像を共同で再作成するためにもう1つのDenseNetデコーダを追加
- NTIRE’19とNTIRE’18の挑戦的なベンチマーク画像データセットで実験
  - AtJ-DH が SOTA
  - 特にNTIRE19