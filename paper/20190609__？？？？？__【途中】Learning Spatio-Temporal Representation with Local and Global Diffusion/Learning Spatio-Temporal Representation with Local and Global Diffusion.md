# Learning Spatio-Temporal Representation with Local and Global Diffusion



## 甲斐コメント

- ビデオ認識のSOTA！

- Globalな特徴を保存してLocalと相互作用させまくる

- （発想はCapsuleNetとにてるのかな？？）

  本文中にも　The diffusion is constructed at every level from bottom to top such that the learnt representations encapsulate a holistic view of content evolution　とある



## Abstract

CNNのフィルタはローカルな操作であり広範囲の依存性を無視する

- ビデオは複雑な時間的変動を伴う情報集約型メディアであるため、この特性は良くない

提案手法：Local and Global Diffusion（LGD）

- ローカル表現とグローバル表現を並行して学習する新しいアーキテクチャで時空間表現学習を促進する
- LGDブロック：2表現の間の拡散をモデル化して相互作用させる
- kernelized classifier：ビデオ認識のために２つの側面からの表現を組み合わせる
- Kinetics-400およびKinetics-600ビデオ分類データセットで検証し高精度（ほぼSOTA）
- グローバルおよびローカル表現の両方の汎化を詳細に検証した



## 1. Introduction

ビデオコンテンツは大きなバリエーションと複雑さを持つため難しい

- 強力で一般的な時空間表現が欲しい

CNNを用いた手法の課題：

- 畳み込み演算が隣接画素の局所的な窓だけを処理すること

  →視野の全体像を十分にとらえることはできない

  →2つの離れたピクセル間の接続は、多数のローカル操作の後にのみ確立され、勾配消失しやすい

提案手法１：Local and Global Diffusion（LGD）

- 広範囲の依存関係を捉えた時空間表現を学習するための新しいアーキテクチャ
- 特徴マップは、局所的変動を表す Local Path と空間的位置における全体的外観を記述する Global Path に分けられる

提案手法２：kernelized classifier

- 両方の経路からの最終的表現をカーネルベース分類器によって結合

→いくつかのビデオ分類ベンチマークでSOTA



## 2. Related Work

とばした



## 3. Local and Global Diffusion

Local and Global Diffusion (LGD)

- LGDは互いに相互作用するローカルパスとグローバルパスを持つセル
- 分類子がローカル表現とグローバル表現を組み合わせる
- これらを用いたＬＧＤ − ２ＤおよびＬＧＤ − ３Ｄを作った

#### 3.1. Local and Global Diffusion Blocks

- 識別的局所表現と大域的表現を組み合わせながら新しい情報を合成するNN

- 特徴マップをローカルパスとグローバルパスに分割

- 2つのパス間の相互作用をモデル化するLGDブロック

  ![キャプチャ](画像\キャプチャ.PNG)



#### 3.2. Local and Global Combination Classifier

2つの表現を組み合わせることによって最終的な予測

ここでは、2つのビデオ間の類似性測定のカーネル化ビューを検討

- 2つのビデオの最後の出力ペア解釈するカーネル

  →ここ難しくて理解できなかった



## 4. Local and Global Diffusion Networks

既存のビデオ表現学習フレームワークのほとんどと容易に統合

![キャプチャ2](画像\キャプチャ2.PNG)



#### 4.1. LGD-2D

- ビデオ表現を学ぶ直接的な方法
- 変換関数Ｆとして２Ｄ畳み込みを直接採用
- LGD-2Dのローカルパスでは、各フレームで独立してCNNする
- 効率的なエンドツーエンドの学習を可能にするために、ビデオをT個のスニペットに一様に分割し、処理のためにスニペットごとに1つのフレームのみを選択
- したがって、LGD-2Dの入力はT個の不連続フレームからなり、グローバルパスはこれらすべてのフレームの全体的な表現を学習
- ネットワークの末端では、ハイブリッド予測を達成するためにlocal and global combination classifierが使用される



#### 4.2. LGD-3D

- ビデオ表現学習のもう1つの主要な分野

- T個の連続したフレームをLGD-3Dネットワークに入力し、3D変換をローカル変換Fで利用します。

- 計算コストが高い

  →spacial spaceにおける２Ｄ畳み込みと時間次元における１Ｄ演算に分解することで擬似３Ｄ畳み込み



#### 4.3. Optimization
- カーネル化分類器によってネットワーク全体を一から訓練することは困難

  →2段階戦略

  - トレーニングの開始時に、組み合わせ分類子を使用せずに基本ネットワークを最適化

    →ローカル表現とグローバル表現を別々に調整

  - その後、2つの表現を合わせた損失関数でトレーニング



## 5. Experiments

#### 5.1. Datasets

Kinetics-400

- 大規模な行動認識ベンチマーク
- 400のアクションカテゴリからの約300Kのビデオで構成
- 300Kのビデオは、トレーニング用、検証用、テスト用にそれぞれ240K、20K、40Kに分割
- 各ビデオは、生のYouTubeビデオから切り取られた10秒の短いクリップ

Kinetics-600

- Kinetics-400データセットの拡張版
- それは600のアクションカテゴリからおよそ480Kのビデオで構成
- 480Kのビデオは、トレーニング用、検証用、テスト用にそれぞれ390K、30K、60Kに分割



#### 5.2. 学習と推論

###### 学習

LGD-2D

- サイズ変更された240×320ビデオフレームからランダムに切り取られた224×224画像として入力
- 1バッチ = 128トリプルフレーム

LGD-3D

- 入力ビデオクリップのサイズは16×112×112に設定され、サイズが16×120×160のサイズ変更された重ならない16フレームクリップからランダムに切り取られる
- 1バッチ = 64クリップ

各フレーム／クリップはデータ増大のために水平方向に沿ってランダムに反転



###### 推論

LGD-2D

- 各トリプルフレームに対して1つのスコアを予測
- ビデオレベルの予測スコアは、10個の一様にサンプリングされたトリプルフレームの平均スコア

LGD-3D

- ビデオレベルの予測スコアは、15個の一様にサンプリングされた16フレームクリップからの平均スコア













## 6. Conclusion

- 統一された方法でローカルとグローバルの表現を学ぶことを目的としたLGDを提案
  - 拡散演算を用いたＬＧＤブロックで局所的表現と全体的表現との間の相互作用を考慮
  - カーネル化分類器も２つの表現からの最終予測を組み合わせるように定式化
- LGD-2DとLGD-3Dを提案
- 大規模なKinetics400とKinetics-600など6つのデータセットでSOTA
- LGDネットワークによって作成された時空間ビデオ表現はデータセットやタスク全体にわたって高度に一般化されている









a