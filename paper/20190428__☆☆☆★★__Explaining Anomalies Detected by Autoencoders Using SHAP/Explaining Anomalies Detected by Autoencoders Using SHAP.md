# Explaining Anomalies Detected by Autoencoders Using SHAP



## 甲斐コメント

注意点：この論文は画像とかでなく単なるベクトルのインプットを想定している

結果は興味ないから、必要になったら読む感じかなあ



## Abstruct

異常検出アルゴリズム

- ドメインエキスパートによる結果検証プロセスが簡単ではない

  →インスタンスが異常である理由の説明があるといいなあ

SHapley Additive explanations (SHAP)

- ゲーム理論に基づくフレームワーク

- AEによって検出された異常を説明するためにSHAPを拡張

  →異常に最も寄与した特徴とそれを設定したものの両方を抽出し視覚的に描写

  →→検出された異常の偽陽性率を最小にする

  →→→ドメインの専門家が異常を理解し、興味を引く異常を除去するのを支援する



## 1. Introduction

異常検出に対するDNN（AEが代表的）

- 利点：エキスパートが以前は認識していなかった異常なインスタンスが含まれている可能性
- 欠点：出力が説明するのが難しい
- 対策：元のモデルの解釈可能な近似
  - LIME：局所モデルを用いた予測を説明するためのモデル不可知論的方法
  - DeepLIFT：全ニューロンの寄与を逆伝播することでモデルを説明するためのモデル特定方法
  - SHAP：一貫性を保証するゲーム理論の特性を考慮して特徴量の重要性を計算する

この論文：AEの出力に見られる異常を説明するためにSHAPを用いる

- 教師なしオートエンコーダモデルの出力を説明する
- この説明を使用して興味深い異常を検出する
- 実世界のデータおよびドメインエキスパートで予備実験を行う

→異常に関する正当化と視覚化を必要とする専門家にとって有益

結果

- 異常を調べることに貢献
- 異常の偽陽性率を最小限に抑える方法についての洞察



## 2. Related Work

SHAPフレームワーク

- 単純化されたバイナリ変数の線形関数の形の説明モデルです。
  f（x）=д（z）=θ0+ im

  - f(x)：元のモデル（このペーパーのAE）
  - g(z)：説明モデル
  - z：簡略化された入力
  - x = hx(z)：元のメソッドへのマッピング関数
  - θ0 = f(hx(0))：すべての単純化された入力なし（零行列？）のモデル出力

- 安定した理論的基礎：ゲーム理論からのShapley値を使用し、各特徴に重要度値（SHAP値）を割り当てる

  - 局所精度：説明モデルはオリジナルモデルの出力と一致しなければならない
  - missingness：元の入力に欠けている特徴は影響を与えてはならない
  - 一貫性：ある特徴をより重視するようにモデルを修正する場合、その特徴の重要性は他の特徴がいかなる場合でも低下しない

  →これらを満たす手法であるため、人間の直感を用いるよりSHAPが優れる

- カーネルSHAPと呼ばれる、SHAP値のモデルにとらわれない近似
  - Shapley値を持つ線形LIMEを使って局所的な説明モデルを構築
  - ローカルモデルは、説明されるインスタンスへの近接性を考慮に入れた解釈可能なモデルを構築するために、データから設定された小さな背景を使用（？）
  - カーネルSHAPを使用するのは、以前のサンプリングベースの推定値よりも元のモデルの評価が少なく、より正確な推定値を提供するため（？）



## 3. 異常の説明

AEでうまく再構築できない特徴の説明

→再構成された特徴のSHAP値を計算し、それらを異常に関連付ける

アルゴリズム

- モデルの重みを取得（再構成部分には興味がないので取得しない？）
- ErrorListから最も高い再構成誤差を持つ特徴を抽出（topM）
- List内の各特徴 x'i について、カーネルSHAPを使用してSHAP値を求める
  - すなわち各特徴 x1, x2 , ...,  xn (xiを除く) の、x'i の予測おける重要度
  - 100個のインスタンスを有するで計算する
- X, i からx'i を予測する
- 特徴に対するSHAPを算出する
- SHAP値を異常に寄与する値に分割



## 6. Conclusion

AEによって明らかにされた異常を説明するために、ゲーム理論に基づいたSHAP値を使用する方法を開発

→高い再構成誤差を有する特徴と異常を寄与する説明的な特徴との間の関係を解明

→→興味深い異常を検出するために使用できることを示した