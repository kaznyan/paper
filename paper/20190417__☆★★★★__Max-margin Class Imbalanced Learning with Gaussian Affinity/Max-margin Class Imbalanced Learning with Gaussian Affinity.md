# Pyramid Mask Detector



## 甲斐コメント

発想としては持っておくべき。必読書といってもよい。

ただし結構ホットなジャンルかつ若干古い感あるので最新のやつがないかチェックしたほうがいい。

代表ベクトルを使うという発想、ベクトルの空間をちゃんと解釈しようとする姿勢、ロスをちゃんと考える切り口が参考になる。



## Abstract

imbalance問題。

単一の式で分類とクラスタリングを同時に実行するハイブリッド損失関数

- ユークリッド空間における「親和性尺度」に基づく
  - 分類境界に対する最大マージン制約
  - 等間隔で等距離のクラスタ中心を確実にする
  - 特徴空間における多様性および識別可能性をサポートするために複数のクラスプロトタイプを学ぶ柔軟性



## 1. Introduction

不均衡なデータセット問題。

→トレーニング用の同時分布モデルp（x、y）とテストセットp（x 0、y 0）の間に不一致

→→再現率の低下



学習とテストの不一致問題。

→分類空間におけるマージンを考えないため

→クラス内変動およびクラス間分離を管理しないため



提案手法：Affinity loss function

- クラス間距離とクラス内分散の両方を低減

  →ユークリッド空間上でクラスのあるべき場所に制約を課す

  →→単純なSoftmaxLossと対照的：メジャーなクラスが多くの空間を占める

- ガウス分布間の類似度を評価している

  →距離が直接的に類似度を表す

  →特徴がクラスタリングされていることになる

- 利点

  - ユークリッド空間上で特徴ベクトルをクラスタリング＆分類する

  - A tractable way to ensure uniformly spaced and equidistant class prototypes (when embedding dimension d and prototype number n are related as: n < d + 1).

  - クラスの "Prototype" が均一に分布する

    →周辺のクラスタが均質な形状を持つ

  - ラベルのノイズやimbalanceに対してrobustになる



## 2. Related Work

#### Imbalance

不均衡なデータセットは複雑な特性を示すため、そのようなデータから学習するには新しい手法とパラダイムを設計する必要があります。既存のクラス不均衡アプローチは次のように分けられます。
2つの主なカテゴリ、1）データレベル、および2）アルゴリズムレベルのアプローチ。データレベルスキームは、例えば、少数派クラスをオーバーサンプリングすること［４１、７、１４、１５、２１］または多数派クラスをアンダーサンプリングすること［２５、３］によって、データの分布を修正する。そのような手法は通常、冗長性および過剰適合（オーバーサンプリングのため）の影響を受けやすく、地層の損失において重大である（アンダーサンプリングのため）。対照的に、アルゴリズムレベルのアプローチは分類器自体を改善する。コスト重視の学習を通じて。そのような方法は、トレーニングデータにおけるクラスの重要性または表現に基づいてクラスに関する事前知識を組み込んでいる[26、38、23]。これらの方法は、SVM [48]、決定木[61]、ブースティング[49]など、さまざまな分類子に適用されてきました。いくつかの研究では、不均衡に取り組むためのコスト重視の分類子の集合をさらに探っています[19、24]。これらのコスト重視の方法に関連する主な課題は、クラス固有のコストが最初に定義されるだけで、トレーニングの過程でコストを動的に更新するメカニズムが欠けていることです。

#### Deepを使った工夫

不均衡なデータから深いモデルを学習するための最近のいくつかの試みがなされている[20、23、5、52、36]。 たとえば、[20]の方法では、最初にニューラルネットワークを使用してトレーニングデータをアンダーサンプリングすることを学び、次にデータのバランスをとるための合成少数オーバーサンプリングTEchnique（SMOTE）ベースの手法を続けます。 [52、36]では、深いモデルは不均衡な分類精度を直接最適化するように訓練されています。 ワン他。 al。 [53]モデルパラメータを過半数からそれより頻度の低いクラスに向かって徐々に転送するためのメタ学習アプローチを提案する。 いくつかの研究[23]、[5]は、コストに敏感なディープネットワークをトレーニングし、それはクラスコストとネットワークの重みを最適化します。 深いモデルを訓練しながらクラスコストを継続的に決定することは、依然としてオープンでやりがいのある研究課題であり、そして大規模データセットからの学習において最適化を扱い難いものにしている[18]。

#### JointLoss

ディープネットワークの分類に使用される一般的な損失関数には、ヒンジ損失、ソフトマックス損失、ユークリッド損失、および対比損失があります[22]。 トリプレット損失は認識とクラスタリングを同時に実行することができますが、大規模なデータセットではトリプレットの組み合わせが膨大になるため、そのトレーニングは法外なものになります[40]。 これらの損失関数は特徴空間において識別可能性を達成するそれらの能力において制限されているので、最近の文献は複数の損失関数の組み合わせを探求している。 この目的のために、[44]は、ソフトマックスとコントラクティブロスの組み合わせが、クラス内のコンパクトさとクラス間の分離可能性を同時に強制することを示しました。 [54]同様の線で、分類とクラスタリングに別々の目的を使用する「中心損失」を提案した。

#### Max Mergin Learning

マージン最大化学習目標は、伝統的に機械学習で使用されてきました。サポートベクターマシンにおけるヒンジ損失は、先駆的な最大マージン学習フレームワークの1つです[16]。最近のいくつかの研究は、最大マージン学習をクロスエントロピー損失関数と統合することを目的としている。これらのうち、Large-margin softmax [33]は、内積類似性に直接クラス間分離性を適用しますが、SphereFace [32]とArcFace [10]は、それぞれ超球多様体に乗法余裕と加法余裕を適用します。特徴空間についての超球の仮定は、結果として生じる損失を顔認識以外の用途には一般化できないものにする。さらに、角度領域でマージンベースの分離を実施することは悪い問題であり、近似または仮定（例えば、単位球）のいずれかを必要とする［１２］。本論文は、クラスタ化と分類を同時に実行し、最大マージン制約の直接施行を可能にする新しい柔軟な損失関数を提案した。次に、提案された損失定式化について説明します。

## 3. Max Margin Learning

提案：不均衡なデータセットを学習するためのハイブリッドマルチタスク定式化

- Softmax Lossが最大マージン学習に不適当

  →分類とクラスタ化を組み合わせる方法

  →→クラス間距離を最大に

  →→クラス内分散を最小に

#### 3.1 Softmax Lossに関する考察

- 最大マージン制約を保証しない

- 投影ベクトルは分類空間において等間隔でない

  →多数派クラスの射影ベクトルは、少数派クラスと比較してより多くの角度空間を占める

  ![キャプチャ](画像\キャプチャ.PNG)

- 少数派クラスの射影ベクトルは多数派クラスと比較して小さい

#### 3.2 複数目的を持つMax Margin Learning

- クラス代表との類似度はガウス類似度で評価

  →マージン最大化制約を直接適用する

  →複数のクラスに対して等間隔の分類境界を持つ

  →学習されたクラスタの分散を制御しクラス内のコンパクト性を高める

  →単一の目的関数で分類とクラスタリングを同時に行う

  →ユークリッド領域で標準的な距離測定を使用して類似性を測定



## 4.Experiments

- 不均衡データセットで検証

  →精度の大幅向上

- ノイズの多いデータセットで検証

  →ノイズ（間違ったデータ）が多くなっても影響が小さい





## 5. Conclusion

新しい損失

→特徴空間内で等サイズの等間隔クラスタを学習する

→クラス分離可能性間の強化およびクラス内変動の低減を可能にする

→ウェイト・ディケイなどの既存の正則化を補完

→追加の計算オーバーヘッドを招くことなく、さまざまなアーキテクチャー・バックボーンに組み込む

→不均衡なデータを含む顔照合および画像分類ベンチマークに対する親和性喪失の有効性