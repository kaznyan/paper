# Dense Scene Information Estimation Network for Dehazing



## 甲斐コメント





## Abstract

Image Dehazing

- 今までの手法例：

  - 周囲光（Ａ）および透過率マップ（ｔ）を推定するディープネットワークを設計
  - ヘイズモデルの逆行列を使用して、デヘイズ画像を推定

- 提案手法：2つの新しいネットワークアーキテクチャを開発

  - At-DH

    - シーン情報を共同で推定するために、共有DenseNetベースのエンコーダと2つの異なるDensetNetベースのデコーダを設計（A, t）

      →物理的パラメータを別々に推定する過去の取り組みとは対照的

  - AtJ-DH

    - At-DHの自然な拡張
    - A, t に加え、曇りのない画像を再作成するためDenseNetベースのデコーダを追加

- 特に濃い曇りによって損なわれた画像を回復するとき、At-DHとAtJ-DHが最先端の選択肢を上回ることを実証



## 1. Introduction

- 古典的なヘイズモデル

  ![キャプチャ](画像\キャプチャ.PNG)

  　I：観測されたぼやけた画像

  　J：真のシーンの放射輝度

  　A：周囲光の強度

  　t：透過率マップ

  さらに

  ![キャプチャ2](画像\キャプチャ2.PNG)

  　β：大気の減衰係数

  　d：シーン深度

  Image Dehazingは、Iに基づいてJを回復するプロセス

  - 式１から、入力として任意の所与のかすんでいる画像に対する解決策の選択に関して複数の可能性がある

  - dに関する情報が提供されれば、Jを復元する作業が簡単だが、この深さ情報が実際に利用可能であることはめったにない

- DLを用いた方法
  - IとJの関係を学習する
  - IとJがセットで手に入らないことも多いので合成データセットを使うことが多い
- ちなみに代表的なDatasetは、NTIRE2019-Dehaze Challenge
- 提案手法
  - At-DH：3位
  - AtJ-DH：1位



## 2. Related Work

ディープラーニングベースの方法

- Cai
  新しいBReLUユニットでtを推定するためにエンドツーエンドのCNNネットワークを導入

- Ren

  tを推定するためのマルチスケールディープニューラルネットワークを提案

- Li

  他の本質的な情報を導入することなく、CNNフレームワークを使えば良いという手法からの脱却

  オールインワンの曇り除去ネットワークを提案（tとAを1つの変数にエンコード）

GANによる方法例

- 推定されたｔと曖昧さ除去された結果との間の相互構造情報を組み入れるために、曇り除去画像および推定された透過マップが本物か偽物かを決定する同時識別器
- DenseブロックおよびResブロックに基づく知覚ピラミッドDNNを使用するマルチスケールDehazing手法
  - 復号化中にシーンのコンテキスト情報を組み込むために復号器内にピラミッドプーリングモジュールを有する符号器 - 復号器構造を含む
- 特別に設計されたFCNに基づいてtとAを推定するためのbi-directional consistency loss
- bilinear CNNを使用して、t, A, Jの相関関係を直接モデル化するbilinear lossを用いてt, Aを推定



## 3. 提案手法

- 共有エンコーダと複数のデコーダで構成
  - DenseNetブロックでエンコーダとデコーダを構築
  - エンコーダは一般的な特徴抽出器
  - デコーダはエンコーダから抽出された特徴に基づいてシーン情報を推定

#### 3.1 Building Block

- エンコーダ
  - Densely Connected Networkの前半部分のみを利用する
- デコーダ
  - 構造はエンコーダと同様だが、より多くのBNを有する
  - 連続するDenseブロック間にResブロックを導入してより高周波の情報を入手する
- Refinementブロック
  - 異なるスケールで出力を改良するために使用
  - 様々なサイズのLocal Average Poolingを行い、1x1poolingでまとめ、アップサンプリングした後に仕上げの畳み込みを行う

#### 3.2 At-DH

At-DHとAtJ-DHの概要

![キャプチャ3](画像\キャプチャ3.PNG)

Lossの計算方法

1. 正しいIを入力してAとtを推論します（Iを作る元となったJもありますがここでは表に出てきません）

2. これを使ってJを計算できます

3. このJを使ってIをさらに逆推論します

4. ここまでで、正しいIとJ、推論されたIとJが出てきたことになります

5. Lossの第一項：正しいIとJ、推論されたIとJがどれだけ離れているかをPixelレベルで見る。

   いわゆるReconstruction Lossであり、二乗誤差で計算する

6. Lossの第二項：正しいIとJ、推論されたIとJが大域的に似ているのかを特徴の比較により見る。

   VGG16により抽出した特徴同士の二乗誤差で計算する（VGGはトレーニングには使っていない部外者っぽい？）

#### 3.3 AtJ-DH

At-DHの弱点は、ほとんどの情報が視覚的に隠されている領域に対してAとtのためのデコーダが十分でないこと

→新しいJデコーダを導入することでAtモデルを拡張

→→かすんでいる観測とそれに対応するGTをペアとして扱うことによってトレーニングされる

→→→トレーニングプロセス中に実際のシーン情報を提供することができる

Lossの計算方法

1. Training Decoder.A and Decoder.t：　At-DHとほぼ同様のLossをAとtについて計算します

2. Training Decoder.J：　Decoder J によって直接生成したJについて、At-DHと同様のLossを計算します

3. Training Decoder.A, Decoder.t, and Decoder.J together：

   Aとtの推論値から計算で求めたJと生成したJがどの程度離れているかを二乗誤差で計算します

4. 足します





