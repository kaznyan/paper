# Rainbow

## Abstruct

- DQNの改善アルゴリズムの組み合わせを調査
- Atariとかで強い

## Introduction

- ダブルDQN（DDQN; van Hasselt、Guez、およびSilver 2016）
  - ブートストラップアクションの選択と評価を分離することにより、Qラーニング（van Hasselt 2010）の過大評価バイアスに対処
- 優先順位付けされたエクスペリエンスの再生（Schaul et al 2015）
  - 学習することが多い遷移をより頻繁に再生することにより、データの効率を向上
- 決闘ネットワークアーキテクチャ（Wang et al 2016）
  - 状態値とAdvantageを個別に表すことにより、アクション全体を一般化する
- A3C（Mnih et al 2016）
  - マルチステップブートストラップターゲット（Sutton 1988; Sutton and Barto 1998）から学習
  - バイアス分散のトレードオフがシフトし、新しく観測された報酬を以前に訪問した状態により早く伝播
- 分布Q学習（Bellemare、Dabney、およびMunos 2017）
  - 平均を推定するのではなく、割引リターンのカテゴリー分布を学習
- Noisy DQN（Fortunato et al 2017）
  - 探索に確率的ネットワークレイヤーを使用
- この論文では、前述のすべての成分を組み合わせるエージェントを研究
  - 実際にほぼ補完的であることを示す
  - データ効率と最終的なパフォーマンスの両方の点でAtariのSotaを得た

## Background

- 価値ベースの強化学習
  - 特定の状態から始まりポリシーπに従う場合に、予想される割引リターンまたは価値の推定値を学習
  - ポリシーを導出する一般的な方法は、アクション値に関してε-greedyに行動すること
    - 遠くまで続く別の行動方針を見つけることが難しい
- DQN（Mnih et al 2015）
  - CNNを使用して、特定の状態St（生のピクセル）の行動価値を近似
  - 再生メモリは最後の100万回の遷移を保持
  - NNのパラメーターは、確率的勾配降下法を使用して損失を最小化することにより最適化
  - ターゲットネットワークは定期的に上書きされる
  - 最適化は、RMSprop（Tieleman and Hinton 2012）を使用

## Extention of DQN

#### Double Q-learning

- 従来のQ学習は、過大評価バイアスの影響を受けていた
- ダブルQ学習（van Hasselt 2010）
  - ブートストラップターゲットに対して実行される最大化
  - 評価からのアクションの選択を分離することにより、この過大評価に対処

#### Prioritized replay

- 従来のDQNは、再生バッファーから均一にサンプリング
- Prioritized Replay
  - 学習すべきことが多い遷移をより頻繁にサンプリング
  - 最後に発生した絶対TDエラーに対する確率ptで遷移をサンプリング

#### Dueling networks

- バリューストリームとアドバンテージストリームの2つの計算ストリーム
- 特別なアグリゲーターによってマージ

#### Multi-step learning

- 通常のQラーニングは次のステップに関する貪欲なアクションを使用
- フォワードビューのマルチステップターゲット（Sutton 1988）
  - nステップ先を評価する学習ができる

#### Distributional RL

- リターンの分布を概算する方法を学ぶ
  - 分布が実際のリターンの分布と密接に一致するようにθを更新することが必要
    - リターン分布がベルマンの方程式を満たすこと

#### Noisy Nets

- 学習初期は、報酬を収集するために多くのアクションを実行する必要がある
- Noisyネット（Fortunato et al 2017）
  - 決定論的でノイズの多いストリームを組み合わせたノイズの多い線形レイヤーを提案
  - 自己アニーリングのような効果がある

## いざ合体

- 次の順で、Rainbowと呼ばれる単一のエージェントに統合
  - 1ステップ⇒マルチステップ
  - 二重Q学習を組み合わせる
  - 比例優先再生
  - Distributionalな評価をする
  - ネットワークアーキテクチャを決闘ネットワークアーキテクチャとする
  - すべての線形層をノイズのあるものに置き換る

## Experimental Methods

- たいしたこと書いてないので飛ばし
- ハイパーパラメータがめっちゃ多いらしい

## Analysis

- A3C、DQN、DDQN、優先DDQN、決闘DDQN、分布DQN、およびNoisy DQN比較
  - データ効率と最終パフォーマンスの両方において、どのベースラインよりも大幅に優れている
- アブレーション：レインボーの完全な組み合わせから1つのコンポーネントを削除
  - 優先順位付けされた再生とマルチステップ学習がRainbowの最も重要なコンポーネント
  - Distributional Qラーニングは比較的寄与が小さいが合ったほうがより良い
  - Noisy Netsはいくつかのゲームで最重要、他ではないほうがよかったりもした
  - 決闘ネットワークやダブルQラーニングはタスクによって役立ったり悪影響だったりする

## Discussion

- DQNのいくつかの改善を統合し、最先端のパフォーマンスを実現する単一の学習アルゴリズムを提案

- 他の技術も統合の余地あり

  - TRPO（Schulman et al 2015）など、純粋なポリシーベースのRLアルゴリズム
  - Actor-Criticの手法（Mnih et al 2016; O’Donoghue et al 2016）
  - eligibility traceにより、nステップのリターンより柔軟な組み合わせが可能
  - エピソード制御（Blundell et al 2016）
  - ブートストラップDQN（Osband et al 2016）
  - カウントベースの探索（Bellemare et al 2016）
  - A3C（Mnih et al 2016）、Gorila（Nair et al 2015）など、環境の並行コピーからの非同期学習
  - 階層型RL

  