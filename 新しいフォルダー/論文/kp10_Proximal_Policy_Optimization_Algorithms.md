# Proximal Policy Optimization Algorithms

## Abstruct

- TRPOのいくつかの利点を受け継いで、PPOと呼ばれる新しい方法を提案
  - 方策勾配法の1つ
  - 環境との相互作用を介してデータをサンプリングし、確率的勾配上昇を使用して「サロゲート（代理）」目的関数を最適化する
  - 複数のエポックを1つとしたミニバッチ更新を可能にする新しい目的関数を提案する
  - 実装がTRPOより簡単で、より一般的で、サンプルの複雑さが優れている
  - ロボットやAtariでテストし、PPOが他のオンライン方策勾配法よりも優れており、全体としてサンプルの複雑さ、単純さ、および時間のバランスが取れていることを示す

## Introduction

- 様々な手法の課題
  - Q学習（関数近似を使用）
    - 多くの単純な問題で失敗
  - Vanilla　Policy Gradient
    - データ効率とロバスト性が不十分
  - TRPO
    - 比較的複雑
    - ノイズ（ドロップアウトなど）やパラメーターの共有（ポリシーと値の関数間、または補助タスク）を含むアーキテクチャとは互換性がない
- この論文では、一次最適化のみを使用しながら、TRPOのデータ効率と信頼できるパフォーマンスを実現するアルゴリズムを導入することにより、現状を改善する
  - 方策のパフォーマンスの悲観的な見積もり（つまり、下限）を形成する、クリッピングされた確率比を持つ新しい目的関数を提案
  - ポリシーを最適化するために、ポリシーからのデータのサンプリングと、サンプリングされたデータに対する最適化のいくつかのエポックの実行を交互に行う
- 代理目的のさまざまな異なるバージョンのパフォーマンスを比較し、クリッピングされた確率比を持つバージョンが最もパフォーマンスが高いことを発見した
- 継続的な制御のタスクで、比較対象のアルゴリズムよりもパフォーマンスが向上
- Atariでは、A2Cよりも（サンプルの複雑さの点で）大幅に優れ、ACERと同程度

## Background: Policy Optimization

#### 方策勾配法

- 方策勾配の推定量を計算し、それを確率的勾配上昇アルゴリズムに適用する

- 同じ軌跡のデータを使用して何度も最適化ステップを実行すると、経験的に、破壊的で大規模な方策の更新につながることがよくあるため、避けられる

#### TRPO

- 「代理」目的関数を最適化する
- 特定の代理目的（平均ではなく状態の最大KLを計算する）が、ポリシーπのパフォーマンスの下限（つまり、悲観的限界）を形成するという事実に基づく
- ペナルティではなくハード制約を使用する理由は、さまざまな問題にわたって、または学習の過程で特性が変化する単一の問題内でさえも機能するβの単一の値を選択することが難しいため
- したがって、TRPOの単調な改善をエミュレートする1次アルゴリズムの目標を達成するには、固定ペナルティ係数βを選択してペナルティ付き目的方程式をSGDで最適化するだけでは十分ではない

## 方針１：クリップされた代理目的関数

- 方策の変更にペナルティを課し、過度に大きい方策の更新を抑える
  - 確率比をクリップし、クリップされたオブジェクトとクリップされていないオブジェクトの最小値を取得
  - 確率比の変化が目的を改善する場合にはそのまま、目的を悪化させる場合にはクリップしていることになる

## 方針２：Adaptive KL Penalty Coefficient

- KLダイバージェンスにペナルティを使用し、ペナルティ係数を適応的に変化させること
- 検証の結果、目的関数のクリップのほうが良いことが分かった

## アルゴリズム

- 代理損失は、一般的な方策勾配法の実装を少し変更するだけで計算できる
  - L_PG の代わりに損失 L_CLIP または L_KL_PEN を単純に作成
  - Advantage関数の分散を減らすために状態価値関数V（s）用いる
  - エントロピー項を追加して十分な探索を確実にする
- RNNでの使用に適した方策勾配法の実装では、Tタイムステップのポリシーを実行し（Tはエピソードの長さよりはるかに短い）、収集したサンプルを更新に使用する
  - タイムステップTに関して働くAdvantageの推測が必要
- PPOは固定長の軌跡セグメントを使用
  - 各反復で、N（並列）アクターのそれぞれがデータのTタイムステップを収集
  - 次に、NTタイムステップで代理損失を作成し、KエポックについてミニバッチSGD（またはAdam ）で最適化

## Experiment

#### 目的関数の比較

- 比較対象
  - No clipping or penalty
  - Clipping
  - KL penalty (fixed or adaptive)
    - ターゲットKL値dtargを使用して、固定ペナルティ係数βまたは適応係数を使用できる
- Clippingが最良のスコア

#### 連続的なドメインでの比較

- 比較対象
  - TRPO
  - クロスエントロピー法（CEM）
  - 適応ステップサイズのVanilla Policy Gradient
  - A2C
  - 信頼領域付きのA2C
- PPOがほぼすべての継続的制御環境で以前の方法よりも優れたスコア
- ショーケース：ヒューマノイドのランニングとステアリング
  - RoboschoolHumanoid
    - 前進運動のみ
  - RoboschoolHumanoidFlagrun
    - ターゲットの位置が200タイムステップごとにランダムに変化するか、ゴールに到達するたびに変化する
  - RoboschoolHumanoidFlagrunHarder
    - ロボットがキューブと 地面から起き上がる

#### Atrariでの比較

- 比較対象
  - A2C
  - ACER

- トレーニング期間全体の平均報酬（高速学習に有利）
  - PPO > ACER > A2C
- トレーニングの最後の100エピソードにわたる平均報酬（最終的なパフォーマンスに有利）
  - ACER > PPO > A2C

## Conclusion

- 確率的勾配上昇の複数のエポックを使用して各ポリシー更新を実行する方策最適化手法
- TPOの安定性と信頼性を備え、実装が簡単で、より一般的な設定に適用可能
- Vanilla Policy Gradientの実装コードを数行変更するだけで可能
- 全体的なパフォーマンス向上