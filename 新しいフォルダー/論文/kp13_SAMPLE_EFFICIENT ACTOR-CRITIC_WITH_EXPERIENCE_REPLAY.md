# SAMPLE EFFICIENT ACTOR-CRITIC WITH EXPERIENCE REPLAY

## Abstruct

- ACER
  - 安定
  - サンプル効率の良い経験再生を備える
  - Actor-Critic
- 離散的な57ゲームのAtariドメイン、いくつかの継続的な制御問題など、困難な環境でもよく機能
- これを達成するために、
  - バイアス修正を伴う切り捨てられた重要度のサンプリング
  - 確率論的Duelingネットワークアーキテクチャ
  - trust region policy optimization method（TRPO？）

## Introduction

- 課題
  - エージェントが環境に作用するたびに、高価なシミュレーションステップが実行される
  - したがって、環境のサンプル効率が重要
- Experience Replay
  - サンプルの相関を減らす
  - サンプル効率を改善する
- Experinece Replayを用いたDQNの欠点
  - 「最適なポリシー」を選ぶのは「決定論的な性質」があるので、「敵対的なドメイン」では制限される
  - 「Q関数」に関して貪欲なアクションを見つけることは、大きなアクションスペースではコストがかかる
- 方策勾配法の欠点
  - 継続的なドメイン、囲碁などの特定のタスクに制限されている
  - A3Cなどは、連続・離散ドメインの両方に適用できるがサンプル効率が悪い

- したがって、

  - 連続・離散ドメインの両方に適用され
  - サンプル効率が良い
  - 安定したActor-Critic

  が課題である

- この論文ではACERを提案し、この問題を初めて解決した。使用したテクニックはは

  - off-policyのRetraceアルゴリズムによる分散の低減（Munos et al、2016）
  - RLエージェントの並列トレーニング（Mnih et al、2016）
  - 「バイアス修正を伴う切り捨てられた重要性のサンプリング」（新提案）
  - 確率論的なDuelingアーキテクチャ（新提案）
  - 効率的なTRPO（新提案）

## BACKGROUND

- A3C
  - Advantage関数を用いて方策を最適化
  - ベースラインとしてVを使用

## 問題設定

- ACER
  - A3Cのoff-policy版と解釈できる
  - 単一のDNNを使用して、πとVを推定
  - A3Cに加え、いくつかの変更と新しいモジュールを導入

## 離散的なACER

- Experience Replayを用いたoff-policy学習は、推定量の変動と安定性を制御することが難しいが、重要度サンプリングなどで軽減されている
- 重要度サンプリングの課題
  - 潜在的に無制限の重要度の重みの積を含むため、非常に高い分散の影響を受ける
  - 重みの積が爆発するのを防ぐため、積を切り捨てる（Wawrzynski（2009））と大きなバイアスに悩まされる可能性がある
  - Degris et al（2012）は、limit関数を用いて勾配の近似を生成
    - Qπを推定できなければ機能しない
    - 限界重要度の重みρtを推定するだけで済むため分散が低くなると予想される
- この論文でのアプローチ
  - Munosら（2016）のRetraceアルゴリズムを採用してQπを推定する
  - 重要度の重みの切り捨て手法を新たに提案し、Degrisらの方法より安定性を改善
  - 計算効率を改善したTRPOを導入

#### マルチステップにおけるQの推定

- Retrace（Munos et al。、2016）を使用
  - オフポリシーのリターンベース手法
  - 分散が小さい
  - ターゲットポリシーの価値関数に収束することが証明されている
- Retraceを使う理由
  - マルチステップのリターンを使用するため、方策勾配の推定におけるバイアスが小さくなる
  - Criticの学習が速くなる
- 再帰的Retrace方程式は推定Qに依存
- それを計算するために、推定値Qθv（xt、at）とポリシーπθ（at | xt）を出力
  - スカラーVθv（xt）ではなく、ベクトルQθv（xt、at）を出力するのがA3Cとの異なる点
  - Vを求めたいときはQの期待値を取ればよい
- 評論家のQθv（xt、at）を学習するために、平均二乗誤差損失のターゲットとしてQret（xt、at）を再度使用し、次の標準勾配でパラメーターθvを更新します。

#### バイアス補正による重要度の切り捨て

- 重要度の重みが大きくなりすぎることを防ぐため工夫をする
  - 重要度の重みのクリッピングにより、勾配推定の分散を制限する
  - 補正項により、推定値を不偏にする
- 補正項のQπ（xt、a）は、NN近似Qθv（xt、at）を使用してモデル化
  - 「truncation with bias correction trick」と呼ばれる結果が得られる
  - 期待値部分は、行動ポリシーµから生成された軌跡のサンプリングで近似
- c =∞のとき、リトレースを使用する、off-policyの方策勾配法と同義になる
- c = 0の場合、Q推定に完全に依存するActor-Criticと同義になる

#### 効率的なTRPO

- 方策更新の安定性を確保するために、ステップごとの変更を制限
- Trust Region Policy Optimization（TRPO）（Schulman et al、2015a）
  - 更新されたポリシーと現在のポリシーの違いを制限
  - 更新ごとにフィッシャーベクトル積を計算するため計算コストが高い
- 新しいTRPOを提案
  - 過去のポリシーの移動平均を表す平均ポリシーネットワークを維持
  - 更新されたポリシーがこの平均から大きく逸脱しないようにする
- ポリシーネットワークを2つの部分に分解
  - 分布fを生成するNN
  - 分布の統計φθ（x）を生成するNN
- まずKLダイバージェンスによって制限された最適化問題を解く
- 次に逆伝播をする
- アルゴリズム
  - ACER on-policyを呼び出して、更新を実行し、軌道を提案
  - 次に、ACER off-policyを呼び出して、いくつかの再生手順を実行
- 実質的にA3Cの修正バージョン
  - VベースラインではなくQベースライン
  - TRPOを使用

## Results on Atari

- 実験
  - 16アクターを並列
  - 各スレッドに最大50 000フレームのサイズのリプレイメモリを追加
  - 単一の学習率を使用しアニーリングしない
  - 20ステップごとに更新を実行
  - c = 10の重要度の重みの切り捨てを使用
  - 信頼領域の更新ありと更新なしの両方で検討
- 「再生率」
  - オンポリシーコンポーネント（A3C）を使用した後、オフポリシーコンポーネントを（ACER）何回使用するか

- 結果
  - Experience Replayの導入によりデータ効率が大幅に向上
  - ACERエージェントはA3Cと同程度の時間で機能

## 「連続」問題への拡張

- RetraceにはQとVの両方の推定が必要だが、連続作用空間でQからVを導出するためには新しい方法が必要
- 詳細は読み飛ばした

## Result on Mujoco

- 実験
  - Atariドメインと異なり、on-policyと併用せず完全にオフポリシー
    - シミュレーターから生成されたエクスペリエンスを使用する（平均4回）
  - サイズが5,000フレームの再生メモリを各スレッドに追加し、50ステップごとに更新
  - 対角標準偏差が0.3に設定されている固定対角共分散の対角ガウスポリシーを使用
- A3Cや重要性サンプリングのベースラインを大きく上回る

## Conclusion

- 継続的および離散的なアクションスペースの両方に対応
- 3つの技術革新
  - 切り捨てられた重要度サンプリングとバイアス補正
  - 確率的決闘ネットワーク
  - 効率的な信頼領域ポリシー最適化
- 特に連続ドメインで非常に効果的