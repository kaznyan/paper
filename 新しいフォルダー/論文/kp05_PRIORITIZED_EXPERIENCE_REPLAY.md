# PRIORITIZED EXPERIENCE REPLAY

## Abstruct

- Experience Replay
  - リプレイメモリから均一にサンプリング = 最初に経験したのと同じ頻度でトランジションを再生
- この研究
  - 経験を優先順位付けするためのフレームワークを開発
  - 重要な遷移をより頻繁に再現し、より効率的に学習できる
  - 41/49のゲームでSOTA

## Introduction

- Online RL
  - 一連の経験を観察しポリシー / 価値関数 / モデル のパラメーターを段階的に更新
- 最も単純な形式：1回の更新後、受信データをすぐに破棄
  - i.i.d（独立同分布）を壊す、強い相関を持つ更新
  - 後で役立つ可能性のあるレアな経験の急速な忘却
- Experience Replay（Lin、1992）はこれらの両方の問題に対処
  - DQN（Mnih et al., 2013; 2015）で実証
    - 大きなスライディングウィンドウリプレイメモリを使用
    - ランダムにサンプリングされ、各遷移を平均8回再訪問
  - 学習に必要なエクスペリエンスの量を減らすのにも役立つ
- この論文
  - 再生するトランジションに優先順位を付けることで効率的かつ効果的にする方法
  - 重要なアイデア
    - いくつかの限定されたトランジションが学習に有効
    - 経験を利用する頻度を変えることができれば効率的に学習できる
  - 「TD誤差の大きいもの」のように、予想される学習が進行しやすい遷移をより頻繁に再生
  - 確率論的優先順位付けと比較して多様性の喪失につながる可能性
  - 重要性サンプリングで修正するバイアスを導入します。 結果のアルゴリズムは堅牢でスケーラブルです。

## Background

- ネズミの脳科学研究
  - Atherton et al. (2015), Olafsdottir et al. (2015), Foster＆Wilson (2006)
    - 海馬でのExperience Replayの証拠を特定
    - 経験のシーケンスが覚醒安静時または睡眠中に再現される
    - 報酬に関連するシーケンスはより頻繁に再生される
  - Singer＆Frank (2009), McNamara et al. (2014)
    - 重大なTDエラーのある経験もより頻繁に再現される
- Value iterationなどの計画アルゴリズムは、更新を適切な順序で優先順位を付けることで効率的になる
  - Prioritized sweeping（Moore＆Atkeson (1993); Andre et al. (1998)）
    - 状態が更新されたとき、Valueの変化に従って優先順位を付ける
  - van Seijen＆Sutton （2013）
    - TDエラーで優先順位を付ける
  - この論文
    - モデルベースの計画ではなくモデルフリーのRLを対象とする
    - さらに、よりロバストな確率的優先順位付けを使用する
- TDエラーは、リソースをどこに集中するかの優先順位付けにも使用される
  - 探索する場所を選択するとき（White et al., 2014）
  - 探索する機能を選択するとき（Geramifard et al., 2011; Sun et al., 2011）。
- 不均衡なデータセットを処理するテクニック
  - 教師あり学習では、リサンプリング、アンダーサンプリング、オーバーサンプリングテクニックなど
  - DeepRLでリサンプリングを使用（Narasimhan et al., 2015）
    - 経験を正の報酬、負の報酬の2通りに分け、それぞれから一定の割合を選んで再生
    - 「ポジティブ/ネガティブ」な経験の自然な概念があるドメインにのみ適用
  - 重要度のサンプリング修正を伴う、エラーに基づく不均一なサンプリング（Hinton（2007））
    - MNISTが3倍高速化
- RL回の動向
  - DQN（Mnih et al., 2013; 2015; Guo et al., 2014; Stadie et al., 2015; Nair et al., 2015; Bellemare et al., 2016）
  - Double DQN（van Hasselt et al., 2016）
  - Dueling DQN（Wang et al., 2015）

## Prioritized Experience Replay

本来は、保存する経験と再生する経験の2つのレベルで設計を選択できる。この論文では後者のみ。

#### そもそも優先順位をつけて効果があるのか？

- 報酬が稀である場合の探索の課題を仮定

  - 優先順位付けの潜在的な利点を理解しやすいため
  - 人工的な「ブラインドクリフウォーク」環境

- この例を使用して、2つのエージェントの学習時間の違いを強調

  - 両方のエージェントは、同じリプレイメモリから取得された遷移に対してQラーニング更新を実行

    - 最初のエージェントはトランジションをランダムに均一に再生

    - 2番目のエージェントはオラクルを呼び出してトランジションに優先順位を付ける

      - オラクルは、現在の状態でのグローバルな損失を最大に削減する遷移を貪欲に選択

        （試行してそのあとの状態を取得することを許容？）

  - オラクルが遷移を適切な順序で選択すると、均一な選択よりも指数関数的に高速化

  - そのようなオラクルは現実的ではないが、均一な再生を改善するメリットがあることを示している

#### TD誤差による優先順位付け

- 各遷移の重要度を測定する基準が必要
  - 理想：RLエージェントが遷移から学習できる量
    - 直接アクセスできない
  - 妥協案：遷移のTD誤差δの大きさ
    - 遷移が「驚くべき」または予期しないものであることを示す
    - 具体的には、価値が次のステップのブートストラップ推定から離れている量
    - SARSAやQ学習などの増分オンラインRLアルゴリズムに特に適する
      - TD誤差を既に計算し、δに比例してパラメーターを更新するため
    - ただし、報酬にノイズが大きい場合、TD誤差は役に立ちにくい場合がある
    - TD誤差を各遷移とともに再生メモリに保存

#### Greedyな優先順位付け

- 最もTD誤差の大きい経験を再生

- ブラインドクリフウォークにおける「均一」「オラクル」「貪欲なTDエラーの優先順位付け」の比較
  - タスクの解決に必要な労力が大幅に削減
    - oracleよりはステップ数が多いが均一と比較して1/10くらいになっている
- ただし、問題点もある
  - リプレイメモリ全体を走査すると高コストなため、TD誤差の更新はリプレイされる遷移のみ
    - 最初のアクセス時にTDエラーの少ない遷移が長時間再生されない
    - ノイズスパイクの影響を受けやすい
    - TD誤差の大きい遷移が頻繁に再生される、過学習しやすい
  - Greedyと均一の中間的な、確率的サンプリング法を導入

#### 確率的サンプリング法

- 検討したもの

  - pi = |δi|
  - ランクベースの優先順位付け（Pは指数αのべき法則分布）

  →どちらもベースラインを大幅に高速化

#### バイアスの焼きなまし

- 学習初期には探索を行うようアニーリングする
  - 具体的には、βと呼ばれる値をβ0（<1）を徐々に1に近づけていく
  - β = 1 のときGreedy

## Atariにおける実験

- 仮説：Prioritized Experience Replayによって、
  - 問題固有のハイパーパラメーター調整が必要ない
  - 効率よく学習できる
- 均一バージョン（DQN, DDQN）と優先順位付き（DDQN）の差を検証
  - 優先順位付きは学習率ηを小さくする（更新の大きい経験を選択するため）
- ほとんどすべてのゲームでパフォーマンスが向上し、全体で学習が2倍速くなった

## Discussion

略

## Conclusion

- 経験からの再生をより効率的に学習できる方法
  - 学習をスピードアップ
  - 最先端のパフォーマンス