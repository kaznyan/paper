# Curiosity-driven Exploration by Self-supervised Prediction

## Abstruct

- 多くの現実のシナリオでは、報酬は非常にまばらである
- 好奇心を定義することは、エージェントに環境を探索させる報酬信号として役立つ
- 好奇心
  - エージェントのSelf-Supervisingの逆ダイナミクスモデルによって学習
  - アクションの結果を予測する能力のエラーとして定式化
  - 重要なことに、エージェントに影響を与えない環境の側面を無視できる定式化
- VizDoomとSuper Mario Brosで評価
  - 報酬がまばらである場合＆報酬がない場合において、探索の効率が高まる
  - 新しい類似シナリオ（同じゲームの新しいレベルなど）への一般化

## Introduction

- 強化学習アルゴリズムは、環境によって提供される報酬を最大化
  - 報酬が継続的に与えられる場合
    - Atariのランニングスコア
    - 到達タスクのロボットアームとオブジェクト間の距離
  - 多くの実際のシナリオでは、報酬は非常にまばらであるか、完全に欠落
- まばらな報酬の場合、目標状態に到達した場合にのみ、ポリシーを更新するための補強を受ける
  - 偶然によって到達するのを待つしかない
- 心理学における「好奇心」
  - 環境を探索し、新しい状態を発見する必要性を説明
  - フランス語の「flaneur」：意図的で目的のない歩行者であり、義務や切迫感に邪魔されない
  - 「将来の報酬を追求するのに役立つかもしれない新しいスキルを学ぶ方法」として一般化
- 好奇心の役割
  - まばらな報酬で課題を解決する
  - エージェントが新しい知識を求めてその環境を探索する
  - 将来のシナリオで役立つかもしれないスキルを学ぶ
- 明示的な目標がなくてもエージェントが一般化可能なスキルを学習できる

##  Curiosity-Driven Exploration

- エージェントは2つのサブシステムで構成されている
  - 好奇心主導の固有の報酬信号 r_it を出力する報酬ジェネレーター
  - その報酬信号を最大化する一連のアクションを出力するポリシー
- 加えて、オプションで環境からの外部の報酬 r_et も受け取ることができる
- rt = r_it + r_et を最大化するようにトレーニング
  - ただし報酬が疎なので r_et はほぼ0
- 方策ベースの方法で全て使用できる（ここではA3Cで検証）

####  Prediction error as curiosity reward

- エージェントが観察する可能性のあるもの

  1. エージェントが制御できるもの

  2. エージェントが制御できず、エージェントに影響を与える可能性のあるもの

  3. エージェントが制御できず、エージェントに影響を与えないもの（どうでも良い情報）

- 1と2をモデル化し、3の影響を受けないようにする必要がある

  ⇒　画像を「エンコードしたもの」を用いて好奇心を計算する

####  Self-supervised prediction for exploration

- ICM：2つの損失を同時に最適化する
  - 逆方向
    - アクションの予測のみに関連する情報をエンコードする特徴空間を学習
    - 状態st（をエンコードしたもの）をもとにt+1での状態（をエンコードしたもの）を予測
  - 順方向
    - 特徴空間から行動を予測
- アクションに影響されない環境機能をエンコードするインセンティブがないため、関係のない情報に対して堅牢になる

##  Experimental Setup

#### Environments

- VizDoom
  - 3-Dナビゲーションタスク
  - アクションスペースは4つ（前進、左移動、右移動、アクションなし）
  - ベストを見つけたとき、または最大2100タイムステップを超えたときに終了
  - ベストを見つけた場合のみ+1というまばらな最終報酬のみを提供し、それ以外の場合はゼロ
- Super Mario Bros
  - 最初のレベルで事前トレーニングを行い、後続のレベルで一般化を示す
  - アクションスペースが広い
    - 複数のボタンを同時に押すことができる
    - ボタンを押している時間が実行されるアクションに影響
  - ゲームからの報酬0で学習

#### Training Details

- 時間依存性をモデル化するために、環境の状態表現として現在のフレームを前の3つのフレームと連結
- Action Repeatを使用（VizDoomでは4回、Marioでは6回）
- A3Cと同様に20のワーカーを非同期的にトレーニング
- ADAMを使用し、そのパラメーターはワーカー間で共有しない

#### A3C architecture

- 畳み込み
  - 4つの畳み込み層
  - 指数線形単位（ELU;（Clevert et al、2015））を各畳み込み層の後に使用
- LSTM
  - 畳み込み層の出力を256ユニットのLSTMに入力
- 全結合層
  - 独立した2つの全結合層を使用して、LSTM特徴表現から値関数とアクションを予測

#### Intrinsic Curiosity Module (ICM) architecture

- 順方向モデルと逆方向モデルで構成
- 逆方向モデル
  - 4つの畳み込み層を使用して、入力状態（st）を288次元の特徴ベクトルφ（st）にマップ
  - φ（st）とφ（st + 1）を単一の特徴ベクトルに連結
  - 全結合（256ユニット）⇒全結合（4ユニット）により行動をアウトプット
- 順方向モデル
  - φ（st）とatを単一のベクトルに連結
  - 全結合（256ユニット）⇒全結合（288ユニット）によりφ（st + 1）を予測

#### Baseline Methods

- ICM + A3C
  - ICMとA3Cを組み合わせた完全なアルゴリズム
- 3つのベースラインと比較
  - Vanilla A3C
    - ε-greedyな探索を行うA3C
  - ICM-pixels + A3C
    - 逆モデルなしのICM
    - ピクセル空間での次の観測を予測する際に、順モデルの損失にのみ依存する報酬
    - 逆モデルレイヤーを削除し、順モデルにデコンボリューションレイヤーを追加
    - 環境の「制御できない部分」に対して不変である埋め込みの学習はできない
    - 埋め込みを学習するよりも性能が悪いことが示される
  - TRPOでトレーニングされた変分情報最大化（VIME）（Houthooft et al、2016）に基づく探索方法

## Experiments

- 検証すること
  - 環境による違い（VizDoomとSuper Mario Bros）
  - ICMの有無
  - 報酬の設定
    - 目標に到達したときのわずかな外的報酬
    - 外部報酬のない探査
  - 新規シナリオへの一般化

#### Sparse Extrinsic Reward Setting

- 報酬のまばらさを変化
  - 初期位置と目標の間の距離を変えることにより、難易度を体系的に変化させた
  - 距離が大きいほど報酬はまばらといえる
    - 「密」：17の可能なスポーン場所のいずれかにランダムにスポーン
    - 「まばら」or「非常にまばら」：最短距離で270および350ステップ離れた部屋にスポーン
  - ベースラインA3Cと比較してICM + A3Cがすべてのケースで優れる
    - 「密」では探索スピードが向上
    - 「まばら」「非情にまばら」ではICMありのみが問題を解決
- ノイズの量を変化
  - ICM-pixelsはテクスチャの変化やホワイトノイズに弱い
  - ICMは完全なスコアを達成するが、ICM-pixelは悪影響が大きい
- TRPO-VIMEとの比較
  - ICMが収束率と精度の両方の点で優れる

#### No Reward Setting

- 優れた探索ポリシーとは、目標がなくてもエージェントが可能な限り多くの状態を訪問できるポリシー
  - マップのどの部分が探索されたかを評価
- 報酬がない環境で良好な探索ができていることが分かった
- VizDoom
  - 外的報酬がない環境で遠くまで探索することを自然に学習
- Mario
  - 外的報酬がない環境でレベル1の30％以上を越える
  - エージェントは敵を殺したり回避したりすることを自動的に発見
    - 敵に殺されるとゲーム空間のごく一部しか見えず、好奇心が飽和するため

#### Generalization to Novel Scenarios

- 空間を探索するときに、学習された動作のどの程度がその特定のスペースに固有であり、新しいシナリオで役立つのに十分一般的であるかをMarioで調査
  - レベル1を学習した後にレベル2, 3を実行できるか
  - レベル1を学習した後にレベル2, 3をFine Tuningで学習できるか
- パターン１：学習したポリシーを「そのまま」新しいものに適用
  - レベル3はレベル1と構造が異なるにもかかわらず、ポリシーはうまく機能
  - レベル2は外観がレベル1と大幅に異なるためうまくいかない
- パターン２：好奇心の報酬のみで微調整することにより、ポリシーを調整
  - レベル2で微調整すると、外観の不一致による問題が解決
  - レベル1で学習してからレベル2を微調整で学習したほうがスコアが高くなる
    - レベル2の方がレベル1より難しいため、移動、ジャンプ、敵を一から殺すなどの基本的なスキルを習得しにくい
    - カリキュラム学習の有効性を表している
  - レベル1で学習してからレベル3を微調整で学習するとスコアが低くなる
    - レベル3がレベル1と似ているため、好奇心が働かず、探索が進行しない
    - 「退屈」に似た現象といえる
- パターン３：いくつかの外因的報酬を最大化するようにポリシーを適合させる
  - 好奇心のみで事前トレーニングされ、外部報酬で微調整されたICMエージェントが、好奇心と外部報酬を同時に最大化するように最初からトレーニングされたICMエージェントよりも速く学習し、より高い報酬を達成する
  - エージェントが環境によって指定された目標を達成する必要がある場合にも、探索行動の学習が有用であることを示す

## Discussion

- 「好奇心」を計算するための報酬信号を生成するメカニズムを提案
- ベースラインA3C、VIME、ベースラインピクセル予測を大幅に上回る
- 優れた一般化をMarioで確認
- 今後の展望
  - 学習した探査行動/スキルを、「原始的/低レベルのポリシー」として使用する
  - レアで重要なイベントをリプレイメモリに保存