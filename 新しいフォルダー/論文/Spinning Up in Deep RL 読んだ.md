# Spinning Up in Deep RL 読んだ

## 導入

#### Spinning Up in Deep RLとは

- OpenAIが作ったドキュメント。

- Deep Learning（DL）を用いた強化学習（RL）について学べる。

  ※Deepでないものは扱ってない？

#### なぜ作られたか

- 標準的なDeep RLの教科書などがまだなく、スタートラインに立つのが大変
- フレームワーク的なものはいくつかあるが、RL初心者にとってはブラックボックス

- 誰に何を提供してくれるのか
  - 誰に：DLを知っているがRLは知らない人
  - 何を：勉強すべきこととその概要


#### カバーしている範囲

- RL用語
- アルゴリズムの種類、基本理論の紹介
- RL研究者に成長する方法について
- 重要な論文（トピック別）
- キーアルゴリズムのコードリポジトリ
- ウォームアップ用の演習



## アルゴリズム概観

#### 紹介されているアルゴリズム

※特に**太字**のものが重要らしい

- On-Policy Algorithms
  - Vanilla Policy Gradient (VPG)
  - Trust Region Policy Optimization (TRPO)
  - **Proximal Policy Optimization (PPO)**
- The Off-Policy Algorithms
  - Deep Deterministic Policy Gradient (DDPG)
  - Twin Delayed DDPG (TD3)
  - **Soft Actor-Critic (SAC)**

#### なぜこのアルゴリズムか

- 信頼性、効率性において最先端に近いため
- アルゴリズム設計、使用の際に生じるトレードオフがわかりやすい

|                | On-Policy Algorithms           | Off-Policy Algorithms             |
| -------------- | ------------------------------ | --------------------------------- |
| 概要           | ポリシーに準拠して方策を決める | ポリシーに準拠しない（Q学習など） |
| 学習方法       | ポリシーを直接最適化           | ベルマンの最適化方程式            |
| サンプル効率   | △                              | 〇                                |
| 安定性         | 〇                             | △（うまくいく保証はない）         |
| アルゴリズム例 | VPG → TRPO → PPO               | DDPG → TD3 → SAC                  |
| 進歩の方向性   | サンプル効率の改善             | 安定性の改善                      |

#### 提供されるコードフォーマット

- PyTorch
- Tensorflow



## 強化学習の基礎

#### コンセプト

- エージェントと環境が相互作用する
  - エージェント
    - ロボット（を制御してるやつ）など
    - **環境**から**状態**を得て、次に実行する**行動**を決定
    - **環境**から**報酬**を得て、報酬の合計を最大化するように学習
  -  環境
    - エージェントがいる（という設定の）世界
    - エージェントが行動すると変化する
    - 自然に変化することもある

#### 用語（レベル1：知っているとある程度のフレームワークを軽くいじれる）

- 状態（States）と観測（Observations）

  - 状態 s：世界の状況に関する「完全な」情報

  - 観測 o：世界の状況に関して「エージェントが観察している」情報（完全でないこともある）

  - ただし、エージェントが状態にアクセスできない場合でも o の代わりに s と表記することがある

    特にエージェントの黄道を決定するという文脈の場合（例：Q(s, a)）

- 行動空間（Action space）

  - 特定の環境で有効なすべての行動のセット
  - 有限の離散値だったり、連続値だったりする

- 方策（Policy）

  - 行動を決定するためにエージェントが使用するルール

    - 確定的な場合（Deterministic Policies）：a<sub>t</sub> = μ(s<sub>t</sub>)
    - 確率的な場合（Stochastic Policies）：a<sub>t</sub> ~ π(･|s<sub>t</sub>)

  - Deep RLの場合、NNの重みでパラメータ化された関数を方策として用いている

    NNのパラメータが存在することを強調するため、θやφを方策の添え字として用いる

    - 確定的な場合：a<sub>t</sub> = μ<sub>θ</sub>(s<sub>t</sub>)
    - 確率的な場合：a<sub>t</sub> ~ π<sub>θ</sub>(･|s<sub>t</sub>)

  - 方策はエージェントの頭脳であるためか、「エージェント」の代わりに「方策」ということがある

  - 確定的、確率的とは？

    - 確定的：この観測に対してこう行動する、というものが定義されている状態

    - 確率的：観測に対する行動を確率で表す

      次の2点が重要

      - サンプリング：確率で表されたものから行動を選択する
      - 対数尤度：ある行動aに対する対数尤度 log(π<sub>θ</sub>(･|s)) の取得

      次の2つの方法を使い分ける

      - 行動空間が離散値の場合：categorical Policies

        - NNを分類器のように使って確率のベクトルを出す

          Input → いくつかのLayer → Sonftmax

      - 行動空間が連続値：diagonal Gaussian Policies

        - ？？？

- 軌跡（Trajectory）

  - **エピソード（episode）**、**ロールアウト（rollout）**とも呼ばれる
  - 状態と行動を連続的にひたすら記録したもの τ = (s<sub>0</sub>, a<sub>0</sub>, s<sub>1</sub>, a<sub>1</sub>, ...)
  - 状態 s<sub>t</sub> の遷移は行動aによってのみ発生、すなわち
    - s<sub>t+1</sub> = f(s<sub>t</sub>, a<sub>t</sub>)
    - s<sub>t+1</sub> = P(･|s<sub>t</sub>, a<sub>t</sub>)

- 報酬（reward）

  - 状態、行動、次の状態によって決定。簡略化できる場合もある

    - r<sub>t</sub> = R(s<sub>t</sub>, a<sub>t</sub>, s<sub>t+1</sub>)
    - r<sub>t</sub> = R(s<sub>t</sub>, a<sub>t</sub>)

    - r<sub>t</sub> = R(s<sub>t</sub>)

  - 軌跡τに対する報酬の合計（リターン）R(τ) の計算方法として

    - finite-horizon undiscounted return：割引率あり、一定ステップだけ足す

    - infinite-horizon discounted return：割引率なし、代わりに全ステップ足す

#### 用語（レベル2：知っているといろんなアルゴリズムを理解できるっぽい）

- 価値関数（value）

  - ある状態（または状態と行動のペア）から開始し、その後特定の方策に従って行動した場合に期待されるリターンを**価値**と定義

  - 値関数はほぼ全てのRLアルゴリズムで使用される

  - 価値関数の使い方

    - On-Policy Value Function V<sup>π</sup>(s)

      状態sから開始し、常に方策πに従って動作する場合に期待されるリターン

    - On-Policy Action-Value Function Q<sup>π</sup>(s, a)

      状態sから開始して任意の行動a（方策と一致しなくてもよい）を実行し、その後は常に方策πに従って動作する場合に期待されるリターン

    - The Optimal Value Function V<sup>＊</sup>(s)

      状態sから開始し、常に**最適な**方策に従って動作する場合に期待されるリターン

    - The Optimal Action-Value Function Q<sup>＊</sup>(s, a)

      状態sから開始して任意の行動a（方策と一致しなくてもよい）を実行し、その後は常に**最適な**方策に従って動作する場合に期待されるリターン

      状態 s における最適な行動とは Q<sup>＊</sup>(s, a) を最大にする a である

- ベルマン方程式

  - 4つの価値関数を結び付けるもの

    基本的な考えかた

    ：ある状態の価値は、①その状態であることから期待される報酬と、②次に遷移する状態の価値である

  - 詳細は省略

- Advantage Functions

  - ある行動が他の行動と比較してどの程度優れているか、というような概念

    A<sup>π</sup>(s, a) = Q<sup>π</sup>(s, a) - V<sup>π</sup>(s)

  - Policy gradient には重要らしい



## 強化学習アルゴリズム分類

#### Model-Free RL と Mode-Based RL

- エージェントが環境のモデルにアクセス（または学習）できるか
  - RLアルゴリズムで最も重要な分岐点
  - 環境のモデルとは、状態遷移と報酬を予測する関数
  - アクセスできるもの：Model-Based RL
    - Alpha zeroなど。
    - エージェントの行動による状態遷移が明白、かつその報酬を予測することも許されている
  - アクセスできないもの：Model-Free RL
    - 迷路など？
    - 状態遷移は、行動してみないとわからない
- Model-Basedの利点
  - エージェントが「先読みできる」こと
    - 順方向に考え、可能な選択肢の範囲で何が起こるか考える
    - その選択肢から何をするか決定できる
    - エージェントは計画から得られた結果を学習したポリシーに抽出できる
  - 結果としてサンプル効率が向上するらしい。環境が見えるべき問題設定なら見えたほうが良い
- Model-Basedの欠点
  - 環境のモデルをエージェントが利用できないことが多い
    - 無理やり経験から学習することもできるが、経験に依存するのでバイアスが大きい
  - モデル学習は根本的に難しい
  - 作りこみに時間がかかる

#### Model-Free RL

- 方法①：方策の最適化
  - 方策を明示的に π<sub>θ</sub>(a|s) として表し、報酬の合計 J(π<sub>θ</sub>) の勾配を登って θ を最適化
  - 各更新は、最新の方策に従って動作している間に収集されたデータのみを使用
  - 方策上のValue関数 V<sup>π</sup>(s) の近似  V<sup>φ</sup>(s) を学習することも含む
    - 方策の更新方法を決めるめに使用
  - アルゴリズム例
    - A2C / A3C：勾配上昇を実行して直接パフォーマンスを最大化
    - PPO：更新の結果として J(π<sub>θ</sub>) が変化する量を見積もる代理目的関数を最大化
- 方法②：行動価値関数Qの最適化
  - 最適な行動価値関数 Q<sup>＊</sup>(s, a) を近似した Q<sup>＊</sup>(s, a) を学習
  - 各更新は、最新の方策とは関係なく、任意の時点で収集されたデータを使用
  - アルゴリズム例
    - DQN：Deep RLの元祖
    - C51：期待値 Q<sup>＊</sup> であるようなリターンの分布を学習（？）
- 方法③：①と②の中間
  - 方策の最適化とQ学習を並立
  - どちらの側の長所と短所の間でも慎重にトレードオフできる
  - アルゴリズム例
    - DDPG：決定論的方策とQ関数を同時に使用して互いに改善する
    - SAC：DDPGの上位互換で、確率的方策、エントロピー正則化などのトリックを使用

#### Model-Free RL

- 方法①：Pure Planning
  - 方策を使わず、代わりにモデル予測制御（MPC）などの純粋な計画手法を使用して行動を選択
  - たとえばMPCは
    - エージェントが環境を観測するたびに、モデルに関して最適な計画を計算
    - 計画は、現在から一定の時間枠を引き継ぐためのすべてのアクションを記述
    - エージェントが計画の最初の行動を実行したら、残りの計画を破棄し、新しい計画をする
  - アルゴリズム例
    - MBMF：deep RLを用いて計画を計算

- 方法②：Expert Iteration
  - Pure planning を改良し、方策を使用するようにした
    - 計画アルゴリズムと、現在の方策からのサンプリングによる行動候補の生成を併用
    - 計画アルゴリズムをエキスパートとして用いることで方策を更新し、計画アルゴリズムに近づける
  - アルゴリズム例
    - ExIt, AlphaZero
      - 甲斐雑記：AlphaZeroは、ガチガチの計画アルゴリズムを別に持つことを前提に、膨大な選択肢から読む範囲を高速かつ粗く絞るアルゴリズムを必要としていたため、この方法を使ったという文脈。これによって作られたNWから候補手を絞り、そこからガチ探索をした。ガチ探索先ではこれとは別に、より厳密に盤面の評価値を計算するNWが動いていたはず。

- その他の応用例

  - Model-Freeアルゴリズムに学習させる架空の経験を生成するのに用いる

    - （架空 + 実際）の経験による増強：MBVE
    - 架空の経験のみによる学習（training in the dream）：World Model

  - Model-Freeアルゴリズムのサブルーチンとして計画手順を組み込む（I2A など）

    - 完全な計画が方策のサイド情報になる

    - 方策は計画をいつどのように使用するかを選択する方法を学習できる

      →モデルの偏りが問題になりにくくなる

      →一部の状態で計画が参考にならない場合、単に無視することを学習できる









a

a

a

a

a

a