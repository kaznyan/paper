# Playing Atari with Deep Reinforcement Learning

## Abstruct

- 強化学習を利用し、高次元の感覚入力から直接制御ポリシーを学習する最初の深層学習モデル
- モデル：CNN
  - 入力：ピクセル
  - 出力：将来の報酬を推定するValue関数
- 強化学習はQ学習
- アーキテクチャや学習アルゴリズムを調整せずに、7つのAtari 2600ゲームにこの方法を適用
  - 6つのゲームでこれまでのすべてのアプローチを上回る
  - そのうち3つのゲームで人間の専門家を上回る

## Introduction

- 映像、音声から直接制御をさせるためDLを使いたい

- 強化学習（RL）とDLを組み合わせるときの課題

  - 教師あり学習の知見がそのまま応用できない

    - RLアルゴリズムは、スパースで、ノイズが多く、遅延するスカラー報酬信号から学習

    - 数千のタイムステップが存在し、アクションと報酬との間の遅延がある

      →画像分類の教師あり学習のような方法を用いることが困難

  - データの質が独特

    - RL学習では相関の高い状態のシーケンスが得られる
    - RLではアルゴリズムが新しい動作を学習すると、データ分布が変化する

- この論文の主張点
  - CNNを用いて生のビデオデータからの制御に成功
  - ネットワークはQ学習アルゴリズムのバリアントでトレーニングし、確率的勾配降下法で重みを更新
  - データ間の相関、データ分布の変化の問題を軽減するために、experience replayを使用
    - 以前の遷移をランダムにサンプリングするもの



## Background

#### 学習の流れ

①エージェントがアクションを選択

②環境（エミュレータ）が内部状態とゲームのスコアを変更

③エージェントがエミュレータからの画像を観測

　ゲームのスコアの変化を表す報酬を受け取る



#### 学習の目的

将来の報酬を最大化する方法でアクションを選択すること。



#### 学習の方法論

- 基本的な考えかたとして、Q<sup>＊</sup>(s, a) を求めれば各状態で最適な行動がわかる
  - Q<sup>＊</sup>(s, a) ：任意の戦略に従うことによって達成可能な最大リターン
  - 求め方：Bellman方程式によりQを反復更新
- 問題点：Qは一般化せずにシーケンスごとに個別に推定される（？）
- 解決策：関数近似により推定する
  - 方法：Qネットワーク
    - 各反復iで変化する一連の損失関数Li（θi）を最小化することによって訓練
- ちょっとこの辺はよくわからない。



## Related Work

#### TD-gammon

- バックギャモンで優れた結果を残した
- Q学習と同様にモデルフリーの強化学習アルゴリズムを使用
- 1つの隠れ層を持つ多層パーセプトロンを使用してValue関数を近似
- チェス、囲碁、チェッカー応用は成功しなかった
  - ダイスロールの確率が状態空間の探索に役立ち、値関数が特にスムーズになるため？

#### TD-gammonの課題とその解決

- モデルフリーのRLと非線形のネットワークを組み合わせるとQ-ネットワークが発散することがある

- ネットワークが発散しない方法が検討されてきた
  - 環境Eの推定にDLを使用
  - 制限付きボルツマンマシンをValue関数やポリシーの推定に使用
  - gradient temporal-difference methodsによって部分的に対処

#### neural fitted Q-learning（NFQ）

- RPROPアルゴリズムを使用して損失関数のシーケンスを最適化し、Qネットワークのパラメーターを更新
- 最初にディープオートエンコーダーを使用してタスクの低次元表現を学習
- 次にこの表現にNFQを適用

#### この論文の研究

- NFQとは異なり、視覚入力からエンドツーエンドで強化学習を適用



## Deep RL

#### TD-Gammonアーキテクチャ

- このアプローチの原点
- DLを用いてValue関数を推定
- 得られた st、at、rt、st + 1、at + 1を用いてネットワークを逐一更新（オンライン学習）

#### Experience replay

- TD-Gammonなどとの相違点
- オンライン学習でなく、各ステップでのエージェントの経験 (st、at、rt、st + 1) を一旦保存しておく
  - 保存されたサンプルのプールから経験をランダムに抽出し、Q学習更新またはミニバッチ更新

- 利点

  - 経験の各ステップが何度も使用されるため、データ効率が向上
  - サンプル間の強い相関関係を避けるためのランダム化が可能
  - 学習する内容が直前の行動に影響されない

- 実際の制限

  - 保存する経験には限界があり、実際には最新N回だけを保存

  - 有益な経験のみを保存するような仕組みもない

    →将来的な伸びしろはこのへん

## Experience

うまくいったよと















あ