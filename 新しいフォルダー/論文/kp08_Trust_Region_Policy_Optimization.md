# Trust Region Policy Optimization

## Abstruct

- 「単調な改善が保証された」ポリシー最適化のための反復的な手順（TRPO）を提唱した
- 理論的に正当化された手順にいくつかの近似を加えた
- natural policy gradient に似ており、NNのような大規模な非線形ポリシーの最適化に有効である
- 様々なタスクにおいて、このアルゴリズムがロバストな性能を発揮することを示した
- ハイパーパラメタのチューニングをほとんど行わずに、単調な改善を与える傾向がある

## Introduction

- 方策最適化のためのアルゴリズムの多くは、大きく3つに分類される
  - 方策反復法(Bertsekas, 2005)
    - 現在の方策の下での値関数の推定と政策の改善を交互に行う
  - 方策勾配法(Peters & Schaal, 2008a)
    - サンプル軌跡から得られる期待リターンの勾配の推定器を使用する
  - derivative-freeの最適化手法
    - クロスエントロピー法(CEM)や共分散行列適応(CMA)など
    - 方策パラメータの観点から最適化されるブラックボックス関数としてリターンを扱う(Szita & Lorincz, 2006)
    - 連続制御問題の制御方策を学習することに成功している(Wampler & Popovic, 2009)
- derivative-free と比較して、勾配を用いる方法はサンプルの困難さが問題となる（らしい）
- 一方、勾配に基づく連続的最適化は複雑で強力な政策を効率的に学習することが可能（らしい）
- この論文の主張
  - あるサロゲート目的関数を最小化することで、自明ではないステップサイズでの方策改善が保証されることを証明する
  - 理論的に正当化されたアルゴリズムに一連の近似を行い、実用的なアルゴリズム（信頼領域政策最適化（TRPO）と呼ぶ）を得る
    - モデルフリー設定で適用可能なシングルパス法
    - シミュレーション（model-based）でのみ可能なvine method
  - これらのアルゴリズムはスケーラブルであり、数万個のパラメータを持つ非線形ポリシーを最適化することができる
  - TRPO法が、生の画像から直接Atariのゲームをプレイするだけでなく、水泳、ホッピング、歩行のための複雑なポリシーを学習できることを示す

## General Stochastic Policies における単調な改善の保証
- mixture policiesにおける収束条件を一般的な確率方策に拡張できることを示す
  - 学習率αをπとπ〜の間の距離尺度に置き換えること、定数を適切に変化させることにより可能
  - 混合政策は実際にはほとんど使われていないので、改善保証を実用的な問題に拡張する上で重要な意味を持つ
- total variation divergence と KL divergence の関係に注目 (Pollard (2000)
- 各反復でMi = Lπi(π) − CDmaxKL(πi, π) を最大化することで、真の目的物ηが非減少であることを保証する
  - min-maxアルゴリズム（Hunter & Lange, 2004）の一種
  - Miは、πiで等しくηをマイナー化するサロゲート関数というもの
- TRPOはペナルティではなく、KL発散に対する制約を使用する近似

## ポリシーの最適化

- 有限サンプル数と任意のパラメータ化の下で、実用的なアルゴリズムである
- 新しい方策と古い方策の間のKLダイバージェンスに対する制約を使う「制約付き更新」
  - ペナルティ係数Cを使うとステップの大きさは小さくなるが、KLダイバージェンスなら大きくできる
- 先行研究(Bagnell & Schneider, 2003; Peters & Schaal, 2008b; Peters et al, 2010)

## Sample-Based Estimation of the Objective and Constraint
- サンプリングしたデータでも推定によって機能する
- 推定を実行するための2つのサンプリング・スキーム
  - シングル・パス
    - 方策勾配推定に一般的に使用されているもの(Bartlett & Baxter, 2011)
  - Vine
    - ロールアウトセットを構築し、ロールアウトセット内の各状態から複数のアクションを実行
    - 方策反復法の文脈で使われる(Lagoudakis & Parr, 2003; Gabillon et al, 2013)

#### Single Path

- s0 ∼ρ0 をサンプリングし、方策 πθold をシミュレートして軌跡s0, a0, s1, a1, ...sT -1, aT -1, sT を生成
  - したがって、q(a|s) = πθold (a|s)
- Qθold (s, a)は、各状態-行動ペア(st, at)において、軌道に沿った将来の報酬の割引合計を取ることによって計算される

#### Vine

- s0 ∼ρ0 をサンプリングし、方策πθi をシミュレートして、いくつかの軌跡を生成
- 次に、これらの軌跡に沿ったN個の状態の部分集合を選択（s1, s2, ... , sN )
  - これを「ロールアウト集合」と呼ぶ
- ロールアウト集合の各状態snに対して、an,k〜q(-|sn)に従ってK個のアクションをサンプリング
  - πθi (-|sn)のサポートを含むサポートを持つq(-|sn)を選択すると、一貫した推定値が得られる
    - 連続的な問題ではq(-|sn) = πθi (-|sn)がうまくいった
    - 離散的な問題では一様分布がうまくいった
- 各状態snでサンプリングされた各アクションan,kに対して、状態snとアクションan,kから始まるロールアウト（短い軌跡）を行うことで、Qˆ θi (sn, an,k)を推定
  - 各ロールアウトのノイズに共通の乱数を用いることで、ロールアウト間のQ値の差の分散を減らす
  - Q値のモンテカルロ推定 (Bertsekas, 2005)
  - 強化学習における共通乱数(Ng & Jordan, 2000)
  - 小さな有限のアクション空間では、可能なすべてのアクションに対してロールアウトを生成できる
  - 大きな状態空間や連続的な状態空間では、重要度サンプリングを用いてサロゲート目的物の推定量を構築できる
  - サンプリングに使用される軌跡が、様々なポイント（ロールアウト集合）で分岐して、いくつかの短い枝（ロールアウト軌跡）になるためVine（つる）
- 利点
  - 局所的な推定値の分散がはるかに小さくなること
- 欠点
  - 推定値のそれぞれについてシミュレータへの呼び出しを多く行うこと
  - システムを任意の状態にリセットできる設定に限定

## アルゴリズム

- Single path または vine を利用したアルゴリズム

  1. シングルパス法またはヴァイン法を用いて、状態-行動ペアのセットを収集し、それらのQ値のモンテカルロ推定値も収集

  2. サンプルを平均化することで、式(14)の推定された目的と制約を構築

  3. 制約付き最適化問題を近似的に解いて、政策のパラメータベクトルθを更新（共役勾配アルゴリズムを使用
     - 勾配の共分散行列を用いず、KL発散のヘシアンを解析的に計算してフィッシャー情報行列(FIM)
     - 分析的推定量は、各状態snでのアクションにわたって積分され、サンプリングされたアクションanには依存しない

- 理論とアルゴリズムとの関係

  - 理論的には、KL発散にペナルティを与えてサロゲート目的物を最適化する。

    アルゴリズムでは、ペナルティの代わりにパラメータδ(KL発散の境界)を用いたハード制約を用いる。

  - Dmax KL(θold, θ)に対する制約は数値最適化や推定が難しいので、代わりにDKL(θold, θ)に制約をかける。

  - アドバンテージ関数の推定誤差を無視している。

## 先行研究との関係性

- 多くの方策更新スキームに統一的な視点を提供する
  - Natural Policy Gradient（Kakade, 2002）
    - Lに対する線形近似とDKL制約に対する二次近似を用いて、式（12）の更新の特殊なケースとして得ることができる
  - Standard Policy Gradiend
    - l2 制約やペナルティを用いることで得ることができる
  - 方策反復更新は
    - Ｌπold（π）の制約なし問題を解くことによって得ることができる
  - 相対エントロピー政策探索(REPS) (Peters et al, 2010)
    - 状態行動マージンp(s, a)を制約する
    - TRPOでは条件式p(a|s)を制約し、コストのかかる内ループの非線形最適化を必要としない
  - Levine and Abbeel (2014)
    - 推定された力学モデルが有効な領域から外れないためにKL発散制約を用いている
    - TRPOはシステムの力学を明示的に推定しない
  - Pirottaら(2013)
    - KakadeとLangfordの結果を基にして一般化し，ここでの結果と異なるアルゴリズムを導出

## Expriment

- 検証すること
  1. シングルパスとVineのサンプリング手順の性能特性 
  2. 固定のペナルティ係数ではなく固定のKLダイバージェンスを使用していることによる影響
  3. TRPOが困難な大規模問題の解決に利用できるか。また、大規模問題に適用した場合、最終的な性能、計算時間、サンプルの複雑さに関して、TRPOは他の手法と比較してどうか

#### Simulated Robotic Locomotion

- MuJoCoシミュレーターを使用
  - ロボットの状態は一般化された位置と速度
  - 制御は関節トルク
  - 駆動不足、高次元、接触による滑らかでないダイナミクスにより、これらのタスクは非常に困難
- すべての実験でδ= 0.01を使用しました。
  実験のセットアップと使用するパラメーターの詳細については、付録の表2を参照してください。
  ニューラルネットワークを使用してポリシーを表現しました。アーキテクチャは図3に示し、詳細は付録Dに示しました。
  標準的なベースラインを確立するために、Barto et al（1983）の定式化に基づいた古典的なカートポールバランス問題も含めました。 。
- 比較
  - 単一パスTRPO
  - Vine TRPO
  - クロスエントロピー法（CEM）
  - gradient-freeの方法（Szita＆Lorincz、2006）
  - CMA（Hansen＆Ostermeier、1996）
  - 古典的な自然ポリシー勾配アルゴリズム（Kakade、2002年）
- 結果
  - 単一パスとVine TRPOはすべての問題を解決し、最良のソリューションを生み出した
  - 自然勾配は、2つの簡単な問題でうまく機能したが、歩行を生成できなかった
  - max KL法は学習が遅いが、最終的な結果は同等
    - 制約の形式が制限されているため
  - TRPOが学習したポリシーのビデオ　http://sites.google.com/site/trpopaper/
- 考察
  - KLペナルティと比較して、KLダイバージェンスを制約する方が堅牢
  - CEMとCMAは微分係数なしのアルゴリズムであるため、パラメーターの数に比例して悪化

#### Playing Games from Images

- 一部のゲームで以前の方法よりも優れていたが、基本的には今までと同等程度のスコア
- 同じポリシー検索方法を適用できる一般性が示された

## Discussion

- 確率的制御ポリシーを最適化するためにTRPOを提案
- KLダイバージェンスペナルティを使用して、ポリシーの予想されるリターンに対するローカル近似を繰り返し最適化する
- アルゴリズムの単調な改善を証明
- KLダイバージェンス制約を組み込んだこのメソッドへの近似により、さまざまな挑戦的な範囲で優れた経験的結果が得られることを示した
- 方策勾配と方策反復を統合するパースペクティブも提供
- ロボットの学習、生の画像を入力とするゲームプレイの両方で成功した頑健な方法