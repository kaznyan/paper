# Deep Reinforcement Learning with Double Q-learning

## Abstruct

- 最近のDQNアルゴリズムには価値の過大評価が問題であることを示す
- 表形式の設定で導入されたDouble Q学習アルゴリズムを一般化して関数近似できることを示す
- DQNアルゴリズムへ適応し、過大評価の減少とパフォーマンス向上を示す

## Introduction

- QラーニングはValueを過大評価する傾向がある
  - 推定された行動価値に対する最大化ステップが含まれているため？
  - 不十分に柔軟な関数近似のため？（ThrunおよびSchwartz、1993）
  - ノイズのため？（van Hasselt、2010、2011）

- 過大評価の影響は？
  - 不確実性に直面した楽観主義は有効（Kaelbling et al., 1996）
  - 過大評価が均一ではない場合や、学習したい状態に集中していない場合は悪影響の可能性がある
  - ThrunとSchwartz（1993）は、これが漸近的にさえ、次善の政策につながる具体的な例を挙げています。
- この論文
  - 過大評価が実際に大規模に発生するかをテスト
    - アクション値が不正確な場合に過大評価が発生する可能性があることを示す
    - 過大評価が以前に認識されていたよりもはるかに一般的である可能性があることを示す
  - 表形式Q関数でDouble DQNを構築
    - より正確な価値の見積もりを生成する
    - いくつかのゲームではるかに高いスコアにつながる
    - DQNの過大評価が実際に貧弱なポリシーにつながり、それらを削減することが有益である
  - DQNを改善することにより、Atariドメインで最先端の結果を得る

## Background

- DDQNの元となる考え方（van Hasselt、2010）
  - アクションの選択と評価の両方に同じ値を使用ししないために、選択を評価から切り離す
  - DQN：自身の古いモデルに相当するTarget Networkを用いてValueを計算
  - DDQN：まったく独立したもう一つのNetworkを用いてValueを計算
- DDQNを使用することで、両方のNetworkで過大評価されていない限りはValueが過大評価されない

## 推定誤差に起因する過大評価

- Q学習の過大評価（Thrun and Schwartz（1993））
  - 行動価値に間隔[-ε, ε]で均一に分布するランダムエラーが含まれている場合に発生する過大評価の大きさを評価
  - （最善ではなく）次善のポリシーにつながる具体的な例を示した
  - 関数近似を使用すると、過大評価が小さなToyProblemにさえ現れることを示した
- van Hasselt（2010）
  - 表形式の表現を使用した場合でも環境のノイズが過大評価につながる可能性があると主張
  - ソリューションとしてDouble Q-learningを提案
- 以下、このセクションでは、環境ノイズ、関数近似、非定常性といったあらゆる種類の推定誤差が過大評価を引き起こす可能性があることをより一般的に示す

#### 環境ノイズ

- Q学習の過大評価には「最小値」もある
  - 行動空間が大きいほど最小値が大きくなる（誤差の影響が無視できない）ことがわかった
- DDQNでは最小値が0にできる（理想的には誤差を限りなく小さくしていける）

#### 関数近似

- 仮定

  - 各状態に10の離散アクションを持つ実数値の連続状態空間
  - 真の最適アクション値は状態のみに依存し、各状態ですべてのアクションが同じ真値を持つ

- 図2

  - 紫の線：真の最適アクション値
  - 緑の線：状態の関数としての単一アクションの近似
  - 関数の柔軟性が不十分である場合、十分な数がサンプリングされていても近似は不正確
  - 関数の柔軟が十分な場合でも、非サンプリング状態では推定誤差が極めて大きい
    - 多くの典型的な学習設定において誤差が大きいことを示している

  - 右のグラフ：誤差の大きさ
    - Q学習ではほとんどの場合過剰評価する
    - DoubleQ-learningでは誤差が非常に小さいうえ、過剰評価だけではなく過小評価もするようになる

#### 非定常性

- 過大評価の誤差は状態やアクションによって異なる
  - どの状態に価値があるかについての誤った相対情報を伝達し、学習したポリシーの品質に直接影響
  - 「不確実性に直面した楽観主義」とは違うので、混同しないよう注意
- DoubleQ-learningを使用して過大評価を減らすと、ポリシーが改善

## DDQN

- Qの最大値を計算する際に、aを選ぶときのネットワークと、aを用いて最大値を計算するときのネットワークを「完全に」分離する
  - DQNは古いネットワークをTargetとして持つので「完全に」は分離していない

## Empirical results

#### 過剰評価

- Double DQNが、より正確な価値の見積もりを生成
- 同時に優れたポリシーも生成

#### ポリシー

- ほとんどのゲームでスコアを改善
- 改善しなかったものでも、過大評価を減らすことは学習の安定性につながる

#### 人間によるスタートに対する頑健性

- 人間の専門家の軌跡から各ゲームでサンプリングされた100の開始点を取得
- Double DQNが明らかに高い中央値と平均値を達成
- 適切な一般化が発生していることが分かった

## Conclusion

- 学習の固有の推定誤差により（Q学習が決定論的であるとしても）大規模問題で楽観的すぎる理由を示した
- Atariゲームで実験し、これらの過大評価は以前から知られていたよりも一般的で深刻であることを示した
- Double Qラーニングを大規模に使用して、この過剰楽観性を削減し、より安定した学習を実現した
- Double DQNと呼ばれる特定の実装を提案した
- Double DQNがより良いポリシーを見つけ、Atari 2600ドメインで最先端の結果を取得することを示した