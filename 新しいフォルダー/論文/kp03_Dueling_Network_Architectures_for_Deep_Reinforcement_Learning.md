# Dueling Network Architectures for Deep Reinforcement Learning

## Abstract

- 新しいネットワーク：Dueling Network
  - 2つの推定量を表す
    - state value function
    - state-dependent action advantage function
  - 基礎となる強化学習アルゴリズムに変更を課すことなく、アクション全体で学習を一般化
  - 多くの行動で価値が類似する場合にも良いポリシー評価ができる
- Atari 2600ドメインでSOTA（当時）



## Introduction

- モデルフリーRLにより適したNNアーキテクチャを提案

  - 既存のRLアルゴリズムと簡単に組み合わせられる

- NW構造

  - state valueとaction advantageを明示的に分離
  - 畳み込み機能は共有
  - 特別なaggregating layerを介して結合し、Qの推定値を生成

  →一般的な単一ストリームでQを表現していたのを、2つのストリームを持つネットワークで置き換える

- 各状態の各アクションの効果を知らずとも、どの状態が価値があるかを評価できる

  - アクションが環境に影響を与えない状態で役立つ

  - これを説明するために「顕著性マップ」を検討

    - 入力に対するトレーニング済の値とアドバンテージストリームのヤコビアンを計算することで生成

    - 自動車ゲームEnduroの例：

      1. 前に車がない場合

         - バリューストリームは道路に注意を払う
         - アドバンテージストリームは注意を払わない

      2. 前に車がある場合

         - バリューストリームは道路に注意を払う

         - アドバンテージストリームは車に注意を払う


## Related Works

- Baird (1993)
  - Value関数とAdvantage関数を独立に維持
  - ベルマン残差更新方程式を2つ持つ
  - Harmon et al. (1995)
    - アドバンテージ更新は、単純な連続時間領域でQ学習よりも速く収束することを示した
  - Harmon＆Baird (1996)
    - アドバンテージ学習アルゴリズムは、単一のアドバンテージ関数のみを表すことを示した

- Sutton et al. （2000）
  - Policy gradient
  - Schulman et al. （2015）
    - 分散を減らすためにオンラインでAdvantage Valueを推定
- Mnih et al. （2015）
  - 深層強化学習でAtariをプレイ
  - その後、様々な研究あり

## BackGround

#### 共通

- エージェントが個別の時間ステップで環境Eと対話する順次意思決定セットアップ
  - Rt, Vπ(s), Qπ(s, a), Q*(s, a)を定義
  - アドバンテージ関数を定義
    - Aπ(s, a) = Qπ(s, a) − Vπ(s)
    - 期待値は0になる
- 関係性
  - 価値関数V：特定の状態sでどれほど良好であるか
  - Q関数は、この状態のときに特定のアクションを選択することの価値
  - アドバンテージ関数A：各アクションの重要度の相対的な値を表す

#### DQN

- 価値関数を近似するために、DQNを使用しQ（s、a;θ）を繰り返し更新して近似する
  - オンラインネットワークQ（s、a;θi）を更新
  - ターゲットネットワークQ（s0、a0;θ-）のパラメーターを固定された反復回数だけ凍結
- Experience Replayはデータ効率を向上させるとともにサンプル間の相関を減少させる

#### Double Deep Q-networks

- Q学習とDQNでは、maxオペレーターは同じ値を使用してアクションの選択と評価の両方を行うため、楽観的な価値の見積もりにつながる
- この問題を軽減するために、DDQNは別のネットワークを利用した評価を行う

#### Prioritized Experience Replay

- 予想される学習の進捗が高い経験の再生確率を高める
- Duelingアーキテクチャでは、均一および優先順位付きリプレイの両方について検証し、どちらも性能向上を確認した

## The Dueling Network Architecture

- Dueling Networkの発想
  - 多くの状態では、各アクションの価値を推定する必要がない
  - 一方、一部の状態では、どの行動をとるかを知ることこそが最も重要
  - 価値関数とAdvantage関数の個別の推定値を提供する2つのストリームを持つ、単一のネットワーク
  - 最後に2つのストリームが結合されて単一の出力Q関数が生成される
    - DDQNやSARSAなどの多くの既存のアルゴリズムでトレーニングできる
    - リプレイメモリの改善、探索ポリシーの改善、本質的な動機付けなど他のアルゴリズムの改善を活用できる
    - 2つのストリームを組み合わせてQ推定値を出力するモジュールには、非常に慎重な設計が必要
- 各関数の性質
  - Qπ（s、a）= Vπ（s）+ Aπ（s、a）
  - Vπ（s）= Ea∼π（s）[Qπ（s、a）]
  - よってEa∼π（s）[Aπ（s、a）] = 0
  - a ∗ = argmaxa0∈AQ（s、a0）の場合、Q（s、a ∗）= V（s）、A（s、a ∗）= 0
- 2つのストリームの集約
  - 1つのストリームはスカラーV（s;θ、β）を出力
  - もう1つのストリームは| A |次元ベクトルA（s、a;θ、α）としてアドバンテージ関数を出力
  - 最後の層として、Q値を計算する層を追加
    - VとAの和を直接計算しない
    - 選んだ行動について、A=0となるよう強要する仕組み
    - 具体的には、①Aのaについてのmax、または②Aのsについての平均、を減算する
      - ②案のほうが安定したので、以降は②案とする

## Experiments

#### 「ポリシー評価タスク」

- コリドー環境 × 5、10、20アクション
  - アクションの数を増やすと、Duelingアーキテクチャは従来のQネットワークよりもパフォーマンスが向上

#### Atari

- Duelingネットワークによる改善を確認
  - ポイント
    - 勾配クリッピングは大事
    - バックプロパゲーション時には最後のたたみ込みレイヤーに入る結合勾配を1 /√2に再スケーリング
- 人間のプレイヤーによる途中経過からの開始に対してもロバスト

#### Prioritized Experience Replayとの組み合わせ

- Atariゲームのパフォーマンスが大幅に向上

## Conclusion

- Duelingアーキテクチャの利点

  - Q関数を効率的に学習できる

  - Q値が更新されるたびに、Vも更新できること

    - 単一ストリームとは対照的

      （1つの行動の値のみが更新される場合、他のすべての行動の値は変更されない）

    - Valueストリームが頻繁に更新され、状態値をより適切に近似できる

- 既存のアルゴリズムの工夫と組み合わせて、劇的な改善をもたらした