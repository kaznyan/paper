# PRIORITIZED EXPERIENCE REPLAY

## Abstruct

- 非同期勾配降下法を使用する新しいフレームワークを提唱した。
  - その中で、4つの非同期標準強化学習アルゴリズムのバリアントを提示
    - なかでもActor-Criticを用いるA3Cが最強だった
  - Atariで当時のSOTA
    - GPUマシンではなく単一のマルチコアCPUで半分の時間でトレーニング
  - 連続的なモーター制御の問題と、3D迷路をナビゲートする新しいタスクで成功
- Actorを並列化することがトレーニングを安定化することを示した



## Introduction

- Off-policyの方法（Q学習など）× Experience Replay がこれまで主流であった
- Experience Replay ではデータのバッチ処理、ランダムサンプリングが可能
  - 利点
    - 非定常性が減少
    - データを非相関化
  - 欠点
    - 多くのメモリと計算を使用
    - off-policyの学習アルゴリズムに限定
- この論文では、非同期に複数のエージェントを並行して実行することでデータを無相関化できることを提唱
  - 特定の時間ステップで、並列エージェントがさまざまな異なる状態を経験するため
- さらなる利点として下記を示した
  - マルチスレッドのCPUで学習可能
  - on-policyの学習アルゴリズムで使用可能
- 成果
  - 2Dと3Dの両方のゲームで成功
    - AtariはSOTA
  - 離散アクションスペースと連続アクションスペースで成功



## Related Work

- Gorila（Nair et al、2015）
  - 分散環境で強化学習エージェントの非同期トレーニングを実行
  - 各プロセスは下記を個別に持つ
    - アクター
    - リプレイメモリ
    - リプレイメモリからデータをサンプリングし、DQN損失の勾配を計算する学習器
  - 勾配は、モデルを更新する中央パラメーターサーバーに非同期的に送信
  - 更新されたポリシーパラメータは、一定の間隔でアクターに送信
- Map Reduceフレームワーク（Li＆Schuurmans、2011）
  - 線形関数近似によるバッチ強化学習法の並列化
    - 大規模なマトリックス演算を高速化するため
    - 経験の収集を並列化したり、学習を安定させたりするためではない
- Sarsaアルゴリズムの並列バージョン（Grounds＆Kudenko、2008）
  - トレーニングを加速するために複数の個別の俳優学習者を使用
- 非同期最適化設定でQlearningの収束特性を調査（Tsitsiklis、1994）
  - 古い情報が常に最終的に破棄され、他のいくつかの技術的前提が満たされている限り、情報の一部が古い場合でもQラーニングの収束が保証されることを示した

## Reinforcement Learning Background

- 価値ベース

  - 行動価値関数はニューラルネットワークなどの関数近似器を用いて表現

    - Q(s, a; θ)

  - Q-learning：Q*(s, a)≒Q(s, a; θ)を直接近似

    - 1ステップ法：次のQおよび報酬のみをヒントにする

      ⇒更新された値Q(s, a)を通して間接的にのみ学習

      　⇒学習プロセスが遅くなる可能性

    - nステップリターン(Watkins, 1989; Peng & Williams, 1996)

      ⇒伝播プロセスがより効率的

- 方策ベース
  - ポリシーπ(a|s; θ)を直接パラメータ化し、E[Rt]上で一般的に近似的な勾配上昇を実行
  - REINFORCE（Williams (1992)）
    - ∇θ log π(at|st; θ)Rt の方向にθを更新
  - ベースライン (Williams, 1992)
    - bt(st)をリターンから差し引くことによって、推定値の分散を減らしながら、不偏性を保つ
    - 結果として得られる勾配は、∇θ log π(at|st; θ) (Rt - bt(st))
  - Actor-Critic (Sutton & Barto, 1998; Degris et al, 2012)
    - ベースライン bt(st)≒V π (st) として勾配の分散推定値を低くすることができる
    - この場合 A(at, st) = Q(at, st)-V (st) を考えていることになる

## Asyncronous RL Framework

- Gorilaに似た非同期アクター学習を使用した
  - Gorila：別々のマシンとパラメータサーバを使用
  - 本手法：1台のマシン上で複数のCPUスレッドを使用
- マルチスレッド非同期バリアントとして下記4種類を検討
  - ワンステップSarsa
  - ワンステップQ-learning
  - n-step Q-learning
  - アドバンテージActor-Critic
- 並行する複数のアクターが、環境の異なる部分を探索している可能性が高いことを示す
  - 多様性を最大化するために、アクターごとに異なる探索ポリシーを明示的に使用することもできる
  - リプレイメモリを使用せず、異なる探索ポリシーを採用する並列アクターを使用
  - 利点
    - 学習の安定化
    - 学習時間の短縮
    - 経験再生に頼らないため、Sarsaやactor-criticのようなオンポリシー強化学習法が使える
- 3つの異なる最適化アルゴリズムを調査
  - momentum SGD
  - 共有統計量のないRMSProp(Tieleman & Hinton, 2012)
  - 共有統計量のあるRMSProp
    - これが最良だった

#### Asynchronous one-step Q-learning

- 各スレッドは各自で環境と対話し、各ステップでQ-学習損失の勾配を計算
- ターゲットネットワークは共有
  - ゆっくりと変化するようにする
- 複数のタイムステップを蓄積してミニバッチ学習をする
- 各スレッドに異なる探索ポリシーを与えることで、ロバスト性を向上
  - 各アクターのεを異なるものに設定する（何らかの分布からサンプリング）ことで実現した

#### Asynchronous one-step Sarsa

- ほぼ同じ

#### Asynchronous n-step Q-learning

- 明示的にnステップのリターンを計算することで前方視点で動作する手法
- まずエピソード終了まで行動してみる
- 次に各状態-行動ペアについて、nステップの勾配を計算
  - 可能な限り長い n 段階のリターンを使用

#### Asynchronous Advantage Actor-Critic (A3C)

- ポリシーπ(at|st; θ)と価値関数V(st; θv)の推定値を保持
  - ポリシーのパラメータθと値関数のパラメータθvは、常にいくつかのパラメータを共有
- nステップリターンを使用
- 方針と価値関数はエピソード終了時に更新
  - 更新は、∇θ0 log π(at|st; θ0 )A(st, at; θ, θv)
- 目的関数にπのエントロピーを追加することで、局所解への早期収束を抑制し、探索を改善
  - (Williams & Peng, 1991)によって提案されたもの

## Experiments

#### 評価プラットフォーム

- Atari 2600
  - ほとんどこれ
- 3DカーレースシミュレータTORCS(Wymann et al, 2013)
- MuJoCo (Todorov, 2015)
- 3D迷路

#### 各タスクの結果

- Atari 2600
  - すべてのAtariドメイン上で学習に成功
    - DQNよりも高速に学習する傾向
    - 一部のゲームでは、1ステップ法よりもnステップ法の方がより速く学習
  - 全体的に、A3Cが最も優れていた
    - わずか1日のトレーニングでDueling Double DQNやGorilaのスコアに到達

- TORCS Car Racing Simulator
  - Atari 2600よりリアルなグラフィック
  - 自分がコントロールする車のダイナミクスを学習する必要
  - A3Cが最も優れていた

- Continuous Action Control Using the MuJoCo Physics Simulator
  - 行動空間が連続
  - A3Cのみ評価
    - 連続的な動作に簡単に拡張できるため
    - 全ての問題において24時間以内のトレーニングで解決

- Labyrinth
  - 新しい 3D 環境（作った？）
    - ランダムに生成された迷路の中で報酬を見つけることを学習する
  - 視覚的な入力のみを用いてランダムな3D迷路を探索するための合理的な戦略を学習した

#### Scalability と データ効率について

- 並列ワーカーの数に応じてスケーリングし、リソースを効率的に利用できる
- 加えて、非同期のワンステップQ-learningとSarsaアルゴリズムは超線形の高速化を示した
  - ワンステップ法では並列アクター学習器を使用した場合、必要なデータが少なくなる
    - ワンステップ法のバイアスを低減するのに、複数スレッドが良い効果を発揮している？

#### ロバスト性

- 「良いスコアにつながる学習率の範囲」がある程度広い（ハイパーパラメータに対してロバスト）
- 最も適切な学習率においてはスコア0のタスクがなかった（タスクに対してロバスト）

## Conclusion

- オフポリシーとオンポリシーの両方で可能
- 離散領域と連続領域の両方で可能
- GPUで学習したDQNよりも高速に学習
- 共有モデルの更新に並列アクターランナーを使用することで、学習プロセスに安定化効果があった
  - 経験再生がなくても安定したオンラインQ学習が可能？
    - ただし経験再生が有能でないという話ではない
    - ここからさらに経験再生を組み込むことでデータ効率を大幅に向上させることができる
    - TORCSのように、環境との対話に時間がかかる領域で有効な可能性
- 他の既存の強化学習手法を組み合わせることで改善の可能性がある
  - 前方視のn-step⇒後方視（Watkins, 1989; Sutton & Barto, 1998; Peng & Williams, 1996）
  - generalized advantage estimation (Schulman et al, 2015b)
  - Q値の過大評価バイアスを低減する手法 (Van Hasselt et al, 2015; Bellemare et al, 2016)
  - 真のオンライン時間差法に関する研究（van Seijen et al, 2015）
  - Dueling Architecture(Wang et al, 2015)
  - spatial softmax (Levine et al, 2015)

## Supplementary

#### momentum SGD vs RMSProp

- ロバスト性は、共有RMSProp > RMSProp > momentum SGD
  - RMSProp
    - 各スレッドが移動平均に関するベクトルgを個別に保持
  - 共有RMSProp
    - スレッド間で移動平均に関するベクトルgを共有し、非同期に更新

#### 実装詳細

- actor-runnerの更新
  - 各スレッドで、5回のアクションごとに更新を実行
  - 最適化には共有RMSPropを使用
- Critic / Q関数 の更新
  - 40000フレームごとに更新を実行
  - 共有ターゲットネットワークとする
- ネットワーク
  - 価値ベース
    - 行動ごとに単一の行動価値を出力
  - Actor-Critic：2つの出力セットを持つ
    - 行動ごとに単一の行動価値を出力するsoftmax出力
    - 価値関数を表す1つの線形出力
- ハイパーパラメータ
  - 割引率 γ=0.99
  - RMSProp減衰係数 α=0.99
  - 探索率
    - 1→0.1（確率0.4）
    - 1→0.01（確率0.3）
    - 1→0.5（確率0.3）
  - Advantage actor-critic のエントロピー正則化 β=0.01

#### 連続行動空間（Mujoco）

- A3Cなら離散行動領域で使用されるものとほぼ同じ実装で可能

- 入力は物理的状態（関節の位置、速度、ターゲットの位置）

  - 3つのタスク（振り子、pointmass2D、グリッパー）については、画像からの直接学習も検討

- ポリシーネットワークの出力層が異なる

  - 離散：出力がSoftmax

  - 連続：出力は2つの実数ベクトル

    - 平均ベクトル μ とスカラー分散 σ^2

      ⇒球状共分散を持つ多次元正規分布と言える

    - μ と σ^2 で決定された正規分布からサンプリングした値で行動を決定

- 連続制御ではポリシーネットワークとバリューネットワークでパラメータを共有していない

- エピソードが数百ステップと長かったため、ブートストラップを使用せず、各エピソードで1回の更新にバッチ処理

- 離散と同様に、探索を促すエントロピーを含む

  - エントロピーの定義が異なる　-1/2 (log(2πσ2 ) + 1)
  - β = 10^-4